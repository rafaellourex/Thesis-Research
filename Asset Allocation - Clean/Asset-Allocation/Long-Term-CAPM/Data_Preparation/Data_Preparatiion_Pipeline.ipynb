{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to prepare data for the Long-Term CAPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTarget (dire, df_target, newTarget = False):\n",
    "    \n",
    "    if newTarget == False:\n",
    "        print('Normal CAPM is being used')\n",
    "        print('Formula = Ri - (Beta*Rm)')\n",
    "        df_target = df_target.rename(columns = {'futAlpha':'excessReturns'})\n",
    "        \n",
    "    if newTarget==True:\n",
    "        print('Adjusted CAPM is being used')\n",
    "        print('Formula = (Ri *abs(Beta))- (*Rm)')\n",
    "        \n",
    "        path = f'{dire}Target_CAPM.csv'\n",
    "        df_target = pd.read_csv(path)\n",
    "        df_target['date'] = pd.to_datetime(df_target['date'],).dt.tz_localize(None)\n",
    "        df_target['fillingDate'] = pd.to_datetime(df_target['fillingDate']).dt.tz_localize(None)\n",
    "        df_target['excessReturns'] = df_target['newAlpha']\n",
    "    \n",
    "    thresh_list = [0,0.05,0.15]\n",
    "    for thresh in thresh_list:\n",
    "        df_target.loc[df_target['excessReturns']>=thresh,f'Target_{thresh}'] = 1\n",
    "        df_target[f'Target_{thresh}'] = df_target[f'Target_{thresh}'].fillna(0)\n",
    "        \n",
    "    return(df_target)\n",
    "\n",
    "\n",
    "def filter_tickers (df):\n",
    "    growth_cols = ['revenue', 'costOfRevenue',\n",
    "       'grossProfit', 'researchAndDevelopmentExpenses', 'costAndExpenses',  'ebitda', 'operatingIncome',\n",
    "            'netIncome', 'eps','quarter','year']\n",
    "    \n",
    "    df_temp = df[growth_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "    \n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "    ticker_eliminate = growth_df.isnull().sum().groupby('symbol').median().sort_values(ascending=False) / len(growth_df)\n",
    "    tickers = ticker_eliminate[ticker_eliminate<=0.05].index\n",
    "    return(tickers)\n",
    "\n",
    "def firstPreparation (df):\n",
    "    #create a copy of the original df to clean\n",
    "    df_clean = df.copy()\n",
    "    #remove tickers that have more than 5% of the periods missing\n",
    "    tickers = filter_tickers(df_clean)\n",
    "    df_clean = df_clean.loc[df_clean.index.get_level_values(0).isin(tickers)]\n",
    "    \n",
    "    return(df_clean)\n",
    "\n",
    "def calculateDiffDays (df):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    Calculate nr of days between filing date and predict date \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df['date_diff'] = (pd.to_datetime(df['futDate']) - pd.to_datetime(df['Date_2']))\n",
    "    days_diff_list = []\n",
    "    for days_diff in df['date_diff'].astype(str).str.split(' '):\n",
    "        days_diff_list.append(days_diff[0])\n",
    "        \n",
    "        \n",
    "    df['date_diff'] = days_diff_list\n",
    "    return(df)\n",
    "    \n",
    "\n",
    "def importData (dire, cloud = False, newTarget=False):\n",
    "    \n",
    "    #import from cloud\n",
    "    if cloud==True:\n",
    "        projectID = 'stockmarket-v0'\n",
    "\n",
    "        query = \"\"\"\n",
    "                SELECT * FROM `stockmarket-v0.stockMarket_dev.quarterlyAlphaDataset` as df\n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "        df = pd.read_gbq(query= query, \n",
    "                        project_id=projectID).set_index(['symbol','date','fillingDate'])\n",
    "        \n",
    "    #import using path\n",
    "    if cloud==False: \n",
    "        path = f'{dire}fundamentalData.csv'\n",
    "        print(path)\n",
    "        df = pd.read_csv(path).set_index(['symbol','date','fillingDate']).drop(columns = 'Unnamed: 0')\n",
    "        \n",
    "    #filter observations based on date\n",
    "    df = df.loc[(df.index.get_level_values(1)>='2003-01-01')&\n",
    "                (df.index.get_level_values(1)<'2023-01-01')]\n",
    "\n",
    "    #reset index\n",
    "    df = df.reset_index()\n",
    "    #set datetime featres\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "    df['fillingDate'] = pd.to_datetime(df['fillingDate']).dt.tz_localize(None)\n",
    "    df = df.set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    #create year and quarter based in fillingDate\n",
    "    df['quarter'] = pd.to_datetime(df.index.get_level_values(2)).quarter\n",
    "    df['year'] = pd.to_datetime(df.index.get_level_values(2)).year\n",
    "    \n",
    "    #generate target\n",
    "    df_target = getTarget(dire, \n",
    "                          df.iloc[:,-5:],\n",
    "                          newTarget=newTarget)\n",
    "\n",
    "    #calculate nr of days between fillingDate and date to predict future returns\n",
    "    df = calculateDiffDays(df)\n",
    "\n",
    "    #drop leakage columns\n",
    "    to_drop = ['futDate',\n",
    "        'futstockPrice', 'futindexPrice', 'futAlpha']\n",
    "\n",
    "    df_clean = firstPreparation(df.drop(columns = to_drop))\n",
    "    \n",
    "    \n",
    "    nr_obs = df_clean.shape[0]\n",
    "    n_cols = len(df_clean.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    return(df_clean, df_target)\n",
    "\n",
    "\n",
    "def genGrowthV2 (df, growth_cols = None):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\n",
    "    Receives:\n",
    "        pandas df with quarterly data \n",
    "        growth_cols - columns where growth will be calculated\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    #get growth columns\n",
    "    if growth_cols == None:\n",
    "        growth_cols = ['revenue', 'costOfRevenue',\n",
    "       'grossProfit', 'researchAndDevelopmentExpenses', 'costAndExpenses',  'ebitda', 'operatingIncome',\n",
    "            'netIncome', 'eps','quarter','year']\n",
    "\n",
    "    #filter data to get only growth columns\n",
    "    df_temp = df[growth_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "\n",
    "    dates_temp = df_temp.reset_index().set_index(['symbol','year','quarter']).unstack('symbol')[['date','fillingDate']]\n",
    "    date = dates_temp.stack('symbol')\n",
    "\n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "    #calculate QoQ growth\n",
    "    QoQ_growth = growth_df.pct_change()\n",
    "    QoQ_growth.replace([np.inf], 1, inplace=True)\n",
    "    QoQ_growth.replace([-np.inf], -1, inplace=True)\n",
    "    QoQ_growth = QoQ_growth.iloc[1:]\n",
    "\n",
    "    QoQ_growth = QoQ_growth.fillna( QoQ_growth.median())\n",
    "    QoQ_growth = QoQ_growth.stack('symbol').fillna(0)\n",
    "\n",
    "    cols = QoQ_growth.columns\n",
    "    for col in cols:\n",
    "            QoQ_growth.rename(columns = {col:f'QoQ_{col}'},\n",
    "                             inplace=True)\n",
    "            \n",
    "    #calculate YoY growth\n",
    "    YoY_growth = growth_df.pct_change(4)\n",
    "    YoY_growth.replace([np.inf], 0, inplace=True)\n",
    "    YoY_growth.replace([-np.inf], 0, inplace=True)\n",
    "    YoY_growth = YoY_growth.iloc[4:]\n",
    "\n",
    "    YoY_growth = YoY_growth.stack('symbol')\n",
    "    YoY_growth = YoY_growth.unstack('symbol')\n",
    "    YoY_growth = YoY_growth.fillna( YoY_growth.median())\n",
    "    YoY_growth = YoY_growth.stack('symbol').fillna(0)\n",
    "\n",
    "\n",
    "    for col in cols:\n",
    "            YoY_growth.rename(columns = {col:f'YoY_{col}'},\n",
    "                             inplace=True)\n",
    "\n",
    "    #merge data\n",
    "    YoY_growth = pd.merge(YoY_growth,\n",
    "                          date.reset_index().rename(columns = {0:'date'}),\n",
    "            on = ['symbol','year','quarter'],\n",
    "            how='left').set_index(['symbol','year','quarter'])\n",
    "\n",
    "\n",
    "    full_growth = pd.merge(YoY_growth,\n",
    "            QoQ_growth,\n",
    "            how = 'left',\n",
    "            on = ['symbol','quarter','year'])\n",
    "    \n",
    "    #remove columns where growth can't be calculated \n",
    "    full_growth = full_growth.loc[full_growth['date'].isnull()==False]\n",
    "    return(full_growth, growth_cols)\n",
    "\n",
    "def gen_TTMV2 (df, ttm_cols = None):\n",
    "    \n",
    "    if ttm_cols == None:\n",
    "        ttm_cols = ['revenue', 'costOfRevenue','grossProfit',\n",
    "                'researchAndDevelopmentExpenses', 'costAndExpenses', 'ebitda', 'operatingIncome',\n",
    "                'netIncome', 'eps']\n",
    "\n",
    "    df_temp = df[ttm_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "\n",
    "    dates_temp = df_temp.reset_index().set_index(['symbol','year','quarter']).unstack('symbol')[['date','fillingDate']]\n",
    "    date = dates_temp.stack('symbol')\n",
    "\n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "\n",
    "    ttm_df = growth_df.rolling(4).sum()\n",
    "    ttm_df = ttm_df.iloc[4:]\n",
    "    ttm_df = ttm_df.stack('symbol')\n",
    "    \n",
    "    cols = ttm_df.columns\n",
    "    for col in cols:\n",
    "            ttm_df.rename(columns = {col:f'{col}_TTM'},\n",
    "                             inplace=True)\n",
    "            \n",
    "    ttm_df = pd.merge(ttm_df,\n",
    "                    date.reset_index().rename(columns = {0:'date'}),\n",
    "            on = ['symbol','year','quarter'],\n",
    "            how='left').set_index(['symbol','year','quarter'])\n",
    "            \n",
    "    return(ttm_df)\n",
    "\n",
    "\n",
    "def fund_df_engineer_0 (fund_df):\n",
    "    #calculate ebit and earnings\n",
    "    fund_df['ebit'] = fund_df['ebitda'] - fund_df['depreciationAndAmortization']\n",
    "    fund_df['earnings'] =fund_df['ebit'] -  (fund_df['interestExpense'] + fund_df['incomeTaxExpense'])\n",
    "    fund_df['totalDividend'] = fund_df['dividend'] * fund_df['weightedAverageShsOut']\n",
    "    return(fund_df)\n",
    "\n",
    "\n",
    "def mergeValue (full_eng):\n",
    "    project_id = 'stockmarket-v0'\n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `stockmarket-v0.stockMarket_dev.enterpriseValue` as df\n",
    "\n",
    "    \"\"\"\n",
    "    df_value = pd.read_gbq(query = query, project_id = project_id)\n",
    "    df_value['date'] = pd.to_datetime(df_value['date'])\n",
    "    full_eng = pd.merge(full_eng.reset_index(),\n",
    "                        df_value,\n",
    "                        on = ['symbol','date'],\n",
    "                        how='left')\n",
    "    return(full_eng)\n",
    "\n",
    "\n",
    "def importIndicators (cloud = False):\n",
    "    #import indicators from bigquery\n",
    "    if cloud ==True:\n",
    "        print('Import technical indicators from cloud')\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `stockmarket-v0.stockMarket_dev.QuarterlyTechnicalIndicators`as df\n",
    "        \"\"\"\n",
    "        projectID = 'stockmarket-v0'\n",
    "        df_indicators = pd.read_gbq(query = query, project_id= projectID)\n",
    "    #import indicators from csv (local machine) \n",
    "    if cloud ==False:\n",
    "        print('Import technical indicators from local disk')\n",
    "        path = r'D:\\Desktop\\Long-Term-Model\\Data\\data_Technicalindicators.csv'\n",
    "        df_indicators = pd.read_csv(path).drop(columns = 'Unnamed: 0')\n",
    "    \n",
    "    df_indicators = df_indicators.loc[df_indicators.MA_21.isnull()==False]\n",
    "    df_indicators['fillingDate'] = pd.to_datetime(df_indicators['fillingDate'])\n",
    "    \n",
    "    return(df_indicators)\n",
    "\n",
    "\n",
    "def eliminateGaps (data):\n",
    "    data['lastFilling'] = data.reset_index().groupby('symbol').shift(1)['fillingDate'].values\n",
    "    data['fillingDiff'] = pd.to_datetime(data['fillingDate']) - pd.to_datetime(data['lastFilling'])\n",
    "    \n",
    "    data = data.loc[data['fillingDiff']<'200 days']\n",
    "    return(data)\n",
    "\n",
    "def replace_inf_with_zero(df):\n",
    "    # Replace infinite values with 0\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def full_FT_Eng (df_clean):\n",
    "    df_eng = df_clean.copy()\n",
    "    \n",
    "    #create columns related to financial ratios\n",
    "    df_eng = fund_df_engineer_0(df_eng)\n",
    "\n",
    "    ttm_cols = ['revenue', 'costOfRevenue','grossProfit',\n",
    "                'researchAndDevelopmentExpenses', 'costAndExpenses', 'ebitda', 'operatingIncome',\n",
    "                'netIncome', 'eps','earnings']\n",
    "\n",
    "    #calculate TTM and growth columns\n",
    "    df_ttm = gen_TTMV2(df_eng,ttm_cols)\n",
    "    df_growth, growth_cols = genGrowthV2(df_eng,)\n",
    "\n",
    "    #create a copy of DF\n",
    "    df_eng_ = df_eng\n",
    "\n",
    "    #sort by symbol and filling date\n",
    "    df_eng_2 = df_eng_.sort_values(['symbol','fillingDate'])\n",
    "\n",
    "\n",
    "    #match indexes of the created datasets with the main df\n",
    "    df_ttm = df_ttm.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    df_growth = df_growth.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    df_eng_2 = df_eng_2.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    \n",
    "    #filter index of main data to account for lagging features\n",
    "    df_eng_2 = df_eng_2.loc[df_eng_2.index.isin(df_ttm.index)]\n",
    "\n",
    "\n",
    "    #merge main data with ttm \n",
    "    full_eng = pd.merge(df_eng_2, df_ttm.drop(columns = ['year','quarter']),\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how = 'right')\n",
    "    \n",
    "    #merge main data with growth \n",
    "    full_eng = pd.merge(full_eng, df_growth.drop(columns = ['year','quarter']),\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how = 'left')\n",
    "    \n",
    "    full_eng = mergeValue(full_eng).set_index(['symbol','fillingDate','date'])\n",
    "    \n",
    "    \n",
    "    #imort technical indicators\n",
    "    df_indicators = importIndicators(cloud=False)\n",
    "    if 'date' in df_indicators.columns:\n",
    "        df_indicators = df_indicators.drop(columns = 'date')\n",
    "    #merge fundamental with technical indicators \n",
    "    full_eng = pd.merge(full_eng.reset_index(),\n",
    "                        df_indicators,\n",
    "                        on=['symbol','fillingDate'],\n",
    "                        how='left').set_index(['symbol','date','fillingDate','year','quarter'])\n",
    "    \n",
    "    full_eng = replace_inf_with_zero(full_eng)\n",
    "    \n",
    "    \n",
    "    \n",
    "    nr_obs = full_eng.shape[0]\n",
    "    n_cols = len(full_eng.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    return(full_eng)\n",
    "\n",
    "\n",
    "def drop_MissmatchedRatios (full_eng):\n",
    "    #drop tickers where financial ratios are missmatched\n",
    "    tickers_toDrop = full_eng.loc[full_eng['priceToSalesRatio'].isnull()==True].reset_index()['symbol'].unique()\n",
    "    full_eng = full_eng.loc[full_eng.index.get_level_values(0).isin(tickers_toDrop)==False]\n",
    "\n",
    "    nr_obs = full_eng.shape[0]\n",
    "    n_cols = len(full_eng.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    return(full_eng)\n",
    "\n",
    "\n",
    "\n",
    "def remove_cols_missing (x_missing, thresh):\n",
    "    '''''''''''''''\n",
    "    receives:\n",
    "        DF\n",
    "        thresh - % threshold\n",
    "    Removes all columns that have more than a certain % missing\n",
    "        \n",
    "    '''''''''''''''\n",
    "\n",
    "    mv_ = (x_missing.isnull().sum() / len(x_missing)).sort_values(ascending=False)\n",
    "    deleted_ = []\n",
    "    for col, value in zip(mv_.index, mv_.values):\n",
    "        if value>=thresh:\n",
    "            x_missing = x_missing.drop(columns=f'{col}')\n",
    "            deleted_.append(col)\n",
    "\n",
    "    print(f'{len(deleted_)} features were removed beucause they has a more than {thresh*100}% of observations missing.')\n",
    "    print(f'Features removed: {deleted_}')\n",
    "    return(x_missing)\n",
    "\n",
    "def interactiveImputer (data, estimator):\n",
    "    from sklearn.experimental import enable_iterative_imputer  \n",
    "    from sklearn.impute import  SimpleImputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    metric_features = data.select_dtypes(include=np.number).columns\n",
    "    cat_features = data.select_dtypes(exclude=np.number).columns\n",
    "    \n",
    "    if len(data)<1000:\n",
    "        sample_size = len(data)\n",
    "    else:\n",
    "        sample_size = 1000\n",
    "    \n",
    "    imputer = IterativeImputer(estimator=estimator)\n",
    "    imputer.fit(data.sample(sample_size)[metric_features])\n",
    "    data_imputed = imputer.transform(data[metric_features])\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=metric_features, index = data.index)\n",
    "    return(pd.concat([data_imputed,data[cat_features]],axis=1), imputer)\n",
    "\n",
    "def missingValues (df_missing):\n",
    "    #get a copy of the df\n",
    "    #fill dividend missing values with 0 \n",
    "    dividend_cols = df_missing.columns[(df_missing.columns.str.contains('dividend'))  \\\n",
    "                                       | (df_missing.columns.str.contains('Dividend'))]\n",
    "    \n",
    "    #for each column in dividends, fill missing values with 0\n",
    "    for col in dividend_cols:\n",
    "        df_missing[col] = df_missing[col].fillna(0)\n",
    "\n",
    "    #for every columns more than  30% of the data missing\n",
    "    thresh = 0.30\n",
    "    df_missing = remove_cols_missing(df_missing.copy(),thresh)\n",
    "    \n",
    "    #get numerical, categoricsal and boolean columns\n",
    "    num_cols = df_missing.select_dtypes(include = np.number).columns\n",
    "    cat_cols = df_missing.select_dtypes(exclude = np.number).columns\n",
    "    bool_cols = df_missing.select_dtypes(include = np.bool).columns\n",
    "    \n",
    "    return(df_missing, num_cols,cat_cols,bool_cols)\n",
    "\n",
    "def imputMissingValues (df):\n",
    "    print('Features with most missing values:')\n",
    "    print(full_eng.isnull().sum().sort_values(ascending = False).iloc[:15])\n",
    "    print(' ')\n",
    "    print('Features with most missing values, in %:')\n",
    "    print(' ')\n",
    "    print((full_eng.isnull().sum()/len(full_eng)).sort_values(ascending = False).iloc[:15])\n",
    "    df_missing, num_cols,cat_cols,bool_cols = missingValues(df)\n",
    "    \n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "    num_cols = df_missing.select_dtypes(include = np.number).columns\n",
    "    cat_cols = df_missing.select_dtypes(include = np.bool_).columns\n",
    "    bool_cols = df_missing.select_dtypes(include = np.object).columns\n",
    "\n",
    "    model = DecisionTreeRegressor(random_state=0,\n",
    "                                  max_depth=3)\n",
    "    \n",
    "    print('Imputing missing data with interactive imputer- Decision Tree is base model')\n",
    "    #imput missing values in numerical features using interactive imputer with decision tree\n",
    "    df_imput_dt, imputer = interactiveImputer(df_missing[num_cols],\n",
    "                                                 model)\n",
    "\n",
    "    #concat df with num features with df with bool and categorical features\n",
    "    df_imput = pd.concat([df_imput_dt, \n",
    "                          df_missing[bool_cols]],\n",
    "                         axis=1)\n",
    "    df_imput = pd.concat([df_imput,\n",
    "                          df_missing[cat_cols]],\n",
    "                         axis=1)\n",
    "    \n",
    "#     fill missing values in sector and industry with unkown\n",
    "    df_imput.loc[df_imput['sector'].isnull()==True,\n",
    "                 'sector'] = 'Unkown'\n",
    "    df_imput.loc[df_imput['industry'].isnull()==True,\n",
    "                 'industry'] = 'Unkown'\n",
    "    \n",
    "    mv_cat = df_imput.isnull().sum().sort_values(ascending=False).iloc[:10]\n",
    "    mv_cat = mv_cat[mv_cat.values>0].index\n",
    "\n",
    "    mv_ = df_imput.select_dtypes(include = np.number).isnull().sum().sum()\n",
    "    print(f'Nr of missing values in numerical features after interactive imputer: {mv_}')\n",
    "    mv_ = df_imput.select_dtypes(exclude = np.number).isnull().sum().sum()\n",
    "    print(f'Nr of missing values in categorical features after interactive imputer: {mv_}')\n",
    "\n",
    "    print(f'Features with missing values {list(mv_cat)}')\n",
    "    \n",
    "    return(df_imput)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_DFModel (df_imput,df_target):\n",
    "    #drop duplicates and set new index\n",
    "    df_full = df_imput.reset_index().drop_duplicates(['symbol','date']).set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    #drop duplicates and set new index\n",
    "    df_target = df_target.reset_index().drop_duplicates(['symbol','date']).set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    df_model = pd.merge(df_full,\n",
    "                        df_target.iloc[:,:],\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how='left')\n",
    "\n",
    "    #drop observations where the target is null \n",
    "    #might be companies where daily price isn't available \n",
    "    #might be targets can't be calculated \n",
    "    df_model = df_model.loc[(df_model['Target_0.05'].isnull()==False)&\n",
    "                           (df_model['excessReturns'].isnull()==False)]\n",
    "    \n",
    "    df_model['date_diff'] = df_model['date_diff'].astype(int)\n",
    "    \n",
    "    return(df_model)\n",
    "\n",
    "#incoherence checking\n",
    "def perform_Incoherence (data):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Receives:\n",
    "        data - pandas df after being merged with Target\n",
    "        \n",
    "    Removes incoherences from the dataset \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    df_inc = data.copy()\n",
    "    #set incoherences\n",
    "    df_inc = df_inc.loc[df_inc['totalDebt']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['totalAssets']>0]\n",
    "    df_inc = df_inc.loc[df_inc['revenue']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['weightedAverageShsOut']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['date_diff']<100]\n",
    "\n",
    "    df_inc_ = data.loc[data.index.isin(df_inc.index)]\n",
    "    init_len = len(data)\n",
    "    pct_rem = np.round(len(df_inc_) / init_len, 3)\n",
    "\n",
    "    print(f'{pct_rem*100}% of the data remained after pre-processing')\n",
    "    \n",
    "    return(df_inc_)\n",
    "\n",
    "\n",
    "def get_Binaries(df):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Create binary variables for the following features:\n",
    "        - priceEarningsRatio\n",
    "        - netDebt\n",
    "        - netIncomeTTM\n",
    "        - netProfitmargin\n",
    "        - major sectors (jf sector is in ['Services', 'Technology', 'Basic Materials', 'Consumer Cyclical'] ) \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    col_name = 'priceEarningsRatio'\n",
    "    new_col = 'sector_priceEarningsRatio_bin'\n",
    "    for sector in df['sector'].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df['sector']==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[ cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[ cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    \n",
    "    col_name = 'netDebt'\n",
    "    new_col = 'netDebt_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    \n",
    "    col_name = 'netIncome_TTM'\n",
    "    new_col = 'netIncome_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.7)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    col_name = 'netProfitMargin'\n",
    "    new_col = 'netProfitMargin_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "\n",
    "\n",
    "\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    sectors = ['Services', 'Technology', 'Basic Materials', 'Consumer Cyclical']\n",
    "    \n",
    "    df.loc[df['sector'].isin(sectors),'mainSector'] = 1\n",
    "    df['mainSector'] = df['mainSector'].fillna(0).astype(bool)\n",
    "    return(df)\n",
    "\n",
    "def gen_categorical (df):\n",
    "    '''''''''''\n",
    "    \n",
    "    receives \n",
    "        pandas df\n",
    "    \n",
    "    Generates Marketcap categories based on it's values\n",
    "    \n",
    "    '''''''''\n",
    "    df['mktcap'] = (df['close'] * df['weightedAverageShsOut'])\n",
    "    \n",
    "    #create marketcap scales\n",
    "    bil = (1000000000)\n",
    "\n",
    "    #if mktcap > 50 billions then mega\n",
    "    df.loc[df['mktcap']>=bil*50,'marketcap'] = 'Mega'\n",
    "\n",
    "    #if mktcap  10 < mktcap <50 billion then large\n",
    "    df.loc[(df['mktcap']>=bil*10) & (df['mktcap']<bil*50),'marketcap'] = 'Large'\n",
    "\n",
    "    #if mktcap  2 < mktcap <10 billion then mid\n",
    "    df.loc[(df['mktcap']<bil*10) & (df['mktcap']>=bil*2),'marketcap'] = 'Mid'\n",
    "\n",
    "    #if mktcap   mktcap <2 billion then small\n",
    "    df.loc[(df['mktcap']<bil*2) ,'marketcap'] = 'Small'\n",
    "\n",
    "    #if mktcap   mktcap <300 millions then micro\n",
    "    df.loc[(df['mktcap']<300) ,'marketcap'] = 'Micro'\n",
    "\n",
    "    mkcap_list = ['Small','Mid']\n",
    "    df.loc[df['marketcap'].isin(mkcap_list),'marketcap_bin'] = 1\n",
    "    df['marketcap_bin'] = df['marketcap_bin'].fillna(0)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def oneHotEncoder (data):\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "    non_metric_features =list(data.select_dtypes(exclude=np.number).set_index(data.index).columns)\n",
    "    ohc = OneHotEncoder(sparse=False)\n",
    "    ohc_feat = ohc.fit_transform(data[non_metric_features])\n",
    "\n",
    "    names = ohc.get_feature_names_out()\n",
    "    \n",
    "    ohc_cat = pd.DataFrame(data =ohc_feat ,columns = names, index = data.index)\n",
    "    return(ohc_cat)\n",
    "\n",
    "\n",
    "def analyzeQuantiles (data,):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    Receives:\n",
    "            pandas dataframe\n",
    "            \n",
    "    Creates a df that analyzes outliers by creating 2 disparity features that measure the level of the outliers.\n",
    "    The fist measure divides de 95% quantile by the 90% and the second divides the mean by the median\n",
    "    \n",
    "    Note: thresholds can be further adjusted\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df_out = data\n",
    "    \n",
    "    quantile_disp = np.round(df_out.describe([.25, .5, .75,0.9,0.95]).T).iloc[:,:-1]\n",
    "    quantile_disp['disparity'] = (quantile_disp['95%'] / quantile_disp['90%']).replace(np.inf,0)\n",
    "    quantile_disp['mean_disp'] = (quantile_disp['mean'] / quantile_disp['50%']).replace(np.inf,0)\n",
    "\n",
    "    quantile_disp = quantile_disp.sort_values(ascending=False,by = 'mean_disp')\n",
    "    display(quantile_disp.iloc[:10])\n",
    "    \n",
    "    return(quantile_disp)\n",
    "\n",
    "\n",
    "#replace outliers with quantie \n",
    "def replaceOutliers (data,upper_thresh, lower_thresh):\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    Receives:\n",
    "            pandas dataframe\n",
    "            \n",
    "    Replaces outliers with the nearest quantile, per example an observation of quantile 99 will be replaced by the quantile 90\n",
    "    \n",
    "    Note: thresholds can be fyrther adjusted\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df_out = data.copy()\n",
    "    \n",
    "    cols = df_out.select_dtypes(include = np.number).columns\n",
    "    rem_cols = df_out.select_dtypes(exclude = np.number).columns\n",
    "    for col in cols:\n",
    "        \n",
    "        upper_quant = df_out[col].quantile(upper_thresh)\n",
    "        lower_quant = df_out[col].quantile(lower_thresh)\n",
    "        \n",
    "        df_out.loc[df_out[col]>upper_quant, col] = upper_quant\n",
    "        df_out.loc[df_out[col]<lower_quant, col] = lower_quant\n",
    "        \n",
    "        \n",
    "    df_out = pd.concat([df_out[cols],\n",
    "                        df_out[rem_cols]],\n",
    "                      axis=1)\n",
    "        \n",
    "    return(df_out)\n",
    "\n",
    "#apply log an cubic root transformation\n",
    "def skewFix (df):\n",
    "    import numpy as np\n",
    "    from scipy.stats import skew\n",
    "    \n",
    "    def replace_inf_with_zero(df):\n",
    "        \n",
    "    # Replace infinite values with 0\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    df_transform = df.copy()\n",
    "    \n",
    "    for col in df_transform.columns:\n",
    "        if df_transform[col].dtype ==np.number:\n",
    "            sk  = skew(df_transform[col])\n",
    "            min_ = df_transform[col].min()\n",
    "            if (sk > 1.5) & (min_ >0):\n",
    "                df_transform[col] = np.log(df_transform[col])\n",
    "\n",
    "            if (sk > 1.5) & (min_ <=0):\n",
    "                df_transform[col] = np.cbrt(df_transform[col])\n",
    "\n",
    "    df_transform = replace_inf_with_zero(df_transform)\n",
    "    return(df_transform)\n",
    "\n",
    "def skewTest (df):\n",
    "    import numpy as np\n",
    "    from scipy.stats import skew\n",
    "    \n",
    "    sk = skew(df)\n",
    "    \n",
    "    df_skew = pd.DataFrame(data = sk,\n",
    "                           index = df.columns,)\n",
    "    return(df_skew)\n",
    "\n",
    "\n",
    "def standSector (data):\n",
    "    quarter_list = data.reset_index()['date'].dt.quarter.unique()\n",
    "    year_list = data.reset_index()['date'].dt.year.unique()\n",
    "    sector_list = data.reset_index()['sector'].unique()\n",
    "    data_norm = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for quarter in quarter_list:\n",
    "            for sector in sector_list:\n",
    "                query_year = data.index.get_level_values(1).year == year\n",
    "                query_quarter = data.index.get_level_values(1).quarter == quarter\n",
    "                query_sector = data.sector == sector\n",
    "                df_temp = data.loc[(query_year) & (query_quarter) & (query_sector)].copy()\n",
    "                if len(df_temp)>0:\n",
    "                    df_quantile = ft_extract.quantile_transform_df(df_temp,n_quantiles=100, random_state=0)\n",
    "\n",
    "                    data_norm = pd.concat([data_norm,df_quantile],\n",
    "                                          axis=0)\n",
    "    return(data_norm)\n",
    "\n",
    "def standQuarter (data):\n",
    "    quarter_list = data.reset_index()['date'].dt.quarter.unique()\n",
    "    year_list = data.reset_index()['date'].dt.year.unique()\n",
    "    \n",
    "    data_norm = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for quarter in quarter_list:\n",
    "            query_year = data.index.get_level_values(1).year == year\n",
    "            query_quarter = data.index.get_level_values(1).quarter == quarter\n",
    "            df_temp = data.loc[(query_year) & (query_quarter) ].copy()\n",
    "            \n",
    "            if len(df_temp)>0:\n",
    "                df_quantile = ft_extract.quantile_transform_df(df_temp,n_quantiles=100, random_state=0)\n",
    "\n",
    "                data_norm = pd.concat([data_norm,df_quantile],\n",
    "                                      axis=0)\n",
    "    return(data_norm)\n",
    "\n",
    "\n",
    "def calcRatios (data):\n",
    "    #calculate current ratio\n",
    "    \n",
    "    #liquidity Ratios\n",
    "    data['currentRatio'] = data['totalCurrentAssets'] / data['totalCurrentLiabilities']\n",
    "    data['currentRatio'] = data['currentRatio'].fillna(0)\n",
    "    \n",
    "    data['quickRatio'] = (data['cashAndCashEquivalents'] + data['shortTermInvestments'] + data['accountsReceivables'] )/ data['totalCurrentLiabilities']\n",
    "    data['quickRatio'] = data['quickRatio'].fillna(0)\n",
    "    \n",
    "    data['cashRatio'] =  data['cashAndCashEquivalents'] / data['totalCurrentLiabilities'] \n",
    "    data['cashRatio'] = data['cashRatio'].fillna(0)\n",
    "    \n",
    "    #debt Ratios\n",
    "    data['debtRatio'] =  data['totalLiabilities'] / data['totalAssets'] \n",
    "    data['debtRatio'] = data['debtRatio'].fillna(0)\n",
    "    \n",
    "    data['debtEquityRatio']= data['totalDebt']/ data['totalEquity']\n",
    "    data['debtEquityRatio'] = data['debtEquityRatio'].fillna(0)\n",
    "    \n",
    "    #profiitability\n",
    "    data['grossProfitMargin'] = data['grossProfit'] / data['revenue']\n",
    "    data['debtEquityRatio'] = data['debtEquityRatio'].fillna(0)\n",
    "    \n",
    "    data['operatingProfitMargin'] = data['operatingIncome'] / data['revenue']\n",
    "    data['operatingProfitMargin'] = data['operatingProfitMargin'].fillna(0)\n",
    "    \n",
    "    data['pretaxProfitMargin'] = data['incomeBeforeTax'] /  data['revenue'] \n",
    "    data['pretaxProfitMargin'] = data['pretaxProfitMargin'].fillna(0)\n",
    "    \n",
    "    data['returnOnAssets'] = data['netIncome'] / data['totalAssets']\n",
    "    data['returnOnAssets'] = data['returnOnAssets'].fillna(0)\n",
    "    \n",
    "    data['returnOnAssets'] = data['netIncome'] / data['totalAssets']\n",
    "    data['returnOnAssets'] = data['returnOnAssets'].fillna(0)\n",
    "    \n",
    "    data['revenuePerShare'] = data['revenue'] / data['numberOfShares']\n",
    "    data['revenuePerShare'] = data['revenuePerShare'].fillna(0)\n",
    "    data = replace_inf_with_zero(data)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "\n",
    "def dropDuplicates(df):\n",
    "    init_len = len(df)\n",
    "    df = df.reset_index().drop_duplicates(['symbol','fillingDate']).set_index(['symbol','date','fillingDate','year','quarter'])\n",
    "    \n",
    "    fin_len = len(df)\n",
    "    \n",
    "    print(f'{fin_len/init_len} of the data remained ')\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def finalPreparation (df_imput, df_target):\n",
    "    #drop columns\n",
    "    if 'alpha' in df_target.columns:\n",
    "        df_target = df_target.drop(columns= 'alpha')\n",
    "    \n",
    "    if 'beta' in df_target.columns:\n",
    "        df_target = df_target.drop(columns= 'beta')\n",
    "        \n",
    "    df_model = gen_DFModel(df_imput,\n",
    "                        df_target)\n",
    "\n",
    "\n",
    "    \n",
    "    #set major columns\n",
    "    target_cols = df_model.columns[df_model.columns.str.contains('Target')]\n",
    "\n",
    "    leakage_cols = [ 'futdate','close',\n",
    "        'futClose', 'futdate', 's&p', 'beta', 'fut_s&p', 'fut_s&p_date',\n",
    "        'fut_beta', 'futReturns', 'futMktReturns','excessReturns', 'Target_0',\n",
    "        'Target_0.05', 'Target_0.15']\n",
    "\n",
    "    growth_cols = ['YoY_costOfRevenue', 'YoY_grossProfit', 'YoY_costAndExpenses',\n",
    "        'YoY_ebitda', 'YoY_operatingIncome', 'YoY_netIncome', 'YoY_eps']\n",
    "\n",
    "    #set columns\n",
    "    ratio_cols = df_model.columns[(df_model.columns.str.contains('Ratio'))]\n",
    "    shares_cols = df_model.columns[(df_model.columns.str.contains('Share'))]\n",
    "    margin_cols = df_model.columns[(df_model.columns.str.contains('Margin'))]\n",
    "    turnover_cols = df_model.columns[(df_model.columns.str.contains('Turnover'))]\n",
    "    dividend_cols = df_model.columns[(df_model.columns.str.contains('dividend'))]\n",
    "    all_colls = [*ratio_cols, *shares_cols, *margin_cols,*turnover_cols,*dividend_cols]\n",
    "    all_colls = set(all_colls)\n",
    "\n",
    "    # df_sector = pd.read_csv(f'{input_path}\\companyInfo_sector.csv').drop(columns = 'Unnamed: 0')\n",
    "    print('Nr of rows:')\n",
    "    print(df_model.shape[0])\n",
    "\n",
    "    #perform incoherence checkinng \n",
    "    df_model = perform_Incoherence(df_model)\n",
    "\n",
    "    ##FEATURE ENGINEER 2\n",
    "    #generate categorical features\n",
    "    df_model = gen_categorical(df_model).reset_index().set_index(['symbol','date',\n",
    "                                                                'fillingDate','year',\n",
    "                                                                'quarter'])\n",
    "\n",
    "\n",
    "    #create marketcap and marketcap categoricalb \n",
    "    # df_model = gen_categorical(df_model)\n",
    "\n",
    "\n",
    "    #apply one hot encoder n sector and industry\n",
    "    df_enc = oneHotEncoder(df_model[['sector','industry']])\n",
    "\n",
    "    #concat with main data\n",
    "    df_model = pd.concat([df_model,\n",
    "                        df_enc],\n",
    "                        axis=1)\n",
    "\n",
    "    #gen binary features\n",
    "    df_model = get_Binaries(df_model)\n",
    "    #remove excess returns higher than 150%\n",
    "    df_model = df_model.loc[df_model['excessReturns']<1.5]\n",
    "\n",
    "    #define bin columns\n",
    "    bin_cols =['Target_0',\n",
    "        'Target_0.05', 'Target_0.15','sector_priceEarningsRatio_bin', 'netDebt_bin',\n",
    "        'netIncome_bin', 'netProfitMargin_bin', 'mainSector']\n",
    "\n",
    "\n",
    "    df_model[bin_cols] = df_model[bin_cols].astype(bool)\n",
    "\n",
    "    bin_cols_ =['sector_priceEarningsRatio_bin', 'netDebt_bin',\n",
    "        'netIncome_bin', 'netProfitMargin_bin', 'mainSector']\n",
    "\n",
    "\n",
    "    if 'index' in df_model.columns:\n",
    "        df_model = df_model.drop(columns = 'index')\n",
    "\n",
    "    df_model = calcRatios(df_model)\n",
    "\n",
    "\n",
    "    nr_obs = df_model.shape[0]\n",
    "    n_cols = len(df_model.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "\n",
    "    #create df skew\n",
    "    df_model_skew = skewFix(df_model)\n",
    "    \n",
    "    df_model = dropDuplicates(df_model)\n",
    "    df_model_skew = dropDuplicates(df_model_skew)\n",
    "\n",
    "    nr_obs = df_model.shape[0]\n",
    "    n_cols = len(df_model.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    \n",
    "    return (df_model, df_model_skew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafae\\Personal\\Github\\Asset-Allocation\\Long-Term-CAPM\\SampleData\\Sample\\\\fundamentalData.csv\n",
      "Normal CAPM is being used\n",
      "Formula = Ri - (Beta*Rm)\n",
      "Nr of observations : 121407\n",
      "Nr of columns : 176\n",
      "Import technical indicators from local disk\n",
      "Nr of observations : 115699\n",
      "Nr of columns : 223\n",
      "Nr of observations : 86516\n",
      "Nr of columns : 223\n",
      "Features with most missing values:\n",
      "dividend               40940\n",
      "totalDividend          40940\n",
      "floatShares            34848\n",
      "operatingCycle         26999\n",
      "cashConversionCycle    26999\n",
      "MA_42                  11544\n",
      "MA_21                  11544\n",
      "Volatility_21          11544\n",
      "excessReturn_21        11544\n",
      "excessReturn_84        11544\n",
      "Volatility_42          11544\n",
      "MA_63                  11544\n",
      "excessReturn_42        11544\n",
      "Volatility_84          11544\n",
      "Volatility_63          11544\n",
      "dtype: int64\n",
      " \n",
      "Features with most missing values, in %:\n",
      " \n",
      "dividend               0.473207\n",
      "totalDividend          0.473207\n",
      "floatShares            0.402793\n",
      "operatingCycle         0.312069\n",
      "cashConversionCycle    0.312069\n",
      "MA_42                  0.133432\n",
      "MA_21                  0.133432\n",
      "Volatility_21          0.133432\n",
      "excessReturn_21        0.133432\n",
      "excessReturn_84        0.133432\n",
      "Volatility_42          0.133432\n",
      "MA_63                  0.133432\n",
      "excessReturn_42        0.133432\n",
      "Volatility_84          0.133432\n",
      "Volatility_63          0.133432\n",
      "dtype: float64\n",
      "3 features were removed beucause they has a more than 30.0% of observations missing.\n",
      "Features removed: ['floatShares', 'cashConversionCycle', 'operatingCycle']\n",
      "Imputing missing data with interactive imputer- Decision Tree is base model\n",
      "Nr of missing values in numerical features after interactive imputer: 0\n",
      "Nr of missing values in categorical features after interactive imputer: 44606\n",
      "Features with missing values ['Date_2', 'symbols', 'finalLink', 'link', 'acceptedDate']\n",
      "Nr of rows:\n",
      "73551\n",
      "99.7% of the data remained after pre-processing\n",
      "Nr of observations : 73192\n",
      "Nr of columns : 382\n",
      "0.9975953656137283 of the data remained \n",
      "0.9975953656137283 of the data remained \n",
      "Nr of observations : 73016\n",
      "Nr of columns : 382\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, r'C:\\Users\\rafae\\Personal\\Github\\Functions\\Data-Science-Functions')\n",
    "sys.path.insert(2, r'C:\\Users\\rafae\\Personal\\Github\\Functions\\Data-Science-Functions\\Functions')\n",
    "\n",
    "import Visualizations_Functions as viz \n",
    "import Classification_CV as class_cv\n",
    "import DataPreparation as preprocess \n",
    "import FeatureExtraction as ft_extract \n",
    "import DataScalling as scalling\n",
    "import FeatureSelection as ft_selection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set preferences\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#set color for graphs\n",
    "color1 = 'royalblue'\n",
    "\n",
    "# input_path = r'E:\\Database\\Reasearch Topic\\Long-Term'\n",
    "# data_path = r'fundamentalData_v2.csv'\n",
    "# target_path = r'CAPM\\QuarterlyTarget_CAPM.csv'\n",
    "# resample_path = r'QuarterlyPrices.csv'\n",
    "\n",
    "#set path\n",
    "dire = r'C:\\Users\\rafae\\Personal\\Github\\Asset-Allocation\\Long-Term-CAPM\\SampleData\\Sample\\\\'\n",
    "\n",
    "df_clean, df_target = importData(dire,\n",
    "                                 cloud=False,\n",
    "                                 newTarget=False)\n",
    "\n",
    "\n",
    "full_eng = full_FT_Eng(df_clean)\n",
    "full_eng = drop_MissmatchedRatios(full_eng)\n",
    "\n",
    "df_imput = imputMissingValues(full_eng)\n",
    "\n",
    "df_model, df_model_skew = finalPreparation (df_imput, df_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- excessReturns corresponds to the Target calculated using the adjusted CAPM formula  \n",
    "- futAlpha corresponds to the Target calculated using the original CAPM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store on computer memory\n",
    "# df_full_path = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation.csv'\n",
    "# df_full_path_skew = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation_Skew.csv'\n",
    "# df_model.to_csv(df_full_path)\n",
    "# df_model_skew.to_csv(df_full_path_skew)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
