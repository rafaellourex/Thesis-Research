{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal CAPM is being used\n",
      "Formula = Ri - (Beta*Rm)\n",
      "Nr of observations : 121407\n",
      "Nr of columns : 176\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, r'C:\\Users\\rafae\\Personal\\Github\\Functions\\Data-Science-Functions')\n",
    "sys.path.insert(2, r'C:\\Users\\rafae\\Personal\\Github\\Functions\\Data-Science-Functions\\Functions')\n",
    "\n",
    "import Visualizations_Functions as viz \n",
    "import Classification_CV as class_cv\n",
    "import DataPreparation as preprocess \n",
    "import FeatureExtraction as ft_extract \n",
    "import DataScalling as scalling\n",
    "import FeatureSelection as ft_selection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set preferences\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def getTarget (df_target, newTarget = False):\n",
    "    \n",
    "    if newTarget == False:\n",
    "        print('Normal CAPM is being used')\n",
    "        print('Formula = Ri - (Beta*Rm)')\n",
    "        df_target = df_target.rename(columns = {'futAlpha':'excessReturns'})\n",
    "        \n",
    "    if newTarget==True:\n",
    "        print('Adjusted CAPM is being used')\n",
    "        print('Formula = (Ri *abs(Beta))- (*Rm)')\n",
    "        path = r'D:\\Desktop\\Long-Term-Model\\Target\\newTarget_CAPM.csv'\n",
    "        df_target = pd.read_csv(path)\n",
    "        df_target['date'] = pd.to_datetime(df_target['date'],).dt.tz_localize(None)\n",
    "        df_target['fillingDate'] = pd.to_datetime(df_target['fillingDate']).dt.tz_localize(None)\n",
    "        df_target['excessReturns'] = df_target['newAlpha']\n",
    "    \n",
    "    thresh_list = [0,0.05,0.15]\n",
    "    for thresh in thresh_list:\n",
    "        df_target.loc[df_target['excessReturns']>=thresh,f'Target_{thresh}'] = 1\n",
    "        df_target[f'Target_{thresh}'] = df_target[f'Target_{thresh}'].fillna(0)\n",
    "        \n",
    "    return(df_target)\n",
    "\n",
    "\n",
    "def filter_tickers (df):\n",
    "    growth_cols = ['revenue', 'costOfRevenue',\n",
    "       'grossProfit', 'researchAndDevelopmentExpenses', 'costAndExpenses',  'ebitda', 'operatingIncome',\n",
    "            'netIncome', 'eps','quarter','year']\n",
    "    \n",
    "    df_temp = df[growth_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "    \n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "    ticker_eliminate = growth_df.isnull().sum().groupby('symbol').median().sort_values(ascending=False) / len(growth_df)\n",
    "    tickers = ticker_eliminate[ticker_eliminate<=0.05].index\n",
    "    return(tickers)\n",
    "\n",
    "def firstPreparation (df):\n",
    "    #create a copy of the original df to clean\n",
    "    df_clean = df.copy()\n",
    "    #remove tickers that have more than 5% of the periods missing\n",
    "    tickers = filter_tickers(df_clean)\n",
    "    df_clean = df_clean.loc[df_clean.index.get_level_values(0).isin(tickers)]\n",
    "    \n",
    "    return(df_clean)\n",
    "\n",
    "def calculateDiffDays (df):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    Calculate nr of days between filing date and predict date \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df['date_diff'] = (pd.to_datetime(df['futDate']) - pd.to_datetime(df['Date_2']))\n",
    "    days_diff_list = []\n",
    "    for days_diff in df['date_diff'].astype(str).str.split(' '):\n",
    "        days_diff_list.append(days_diff[0])\n",
    "        \n",
    "        \n",
    "    df['date_diff'] = days_diff_list\n",
    "    return(df)\n",
    "    \n",
    "\n",
    "def importData (cloud = False, newTarget=False):\n",
    "    \n",
    "    #import from cloud\n",
    "    if cloud==True:\n",
    "        projectID = 'stockmarket-v0'\n",
    "\n",
    "        query = \"\"\"\n",
    "                SELECT * FROM `stockmarket-v0.stockMarket_dev.quarterlyAlphaDataset` as df\n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "        df = pd.read_gbq(query= query, \n",
    "                        project_id=projectID).set_index(['symbol','date','fillingDate'])\n",
    "        \n",
    "    #import using path\n",
    "    if cloud==False: \n",
    "        df = pd.read_csv(path).set_index(['symbol','date','fillingDate']).drop(columns = 'Unnamed: 0')\n",
    "        \n",
    "    #filter observations based on date\n",
    "    df = df.loc[(df.index.get_level_values(1)>='2003-01-01')&\n",
    "                (df.index.get_level_values(1)<'2023-01-01')]\n",
    "\n",
    "    #reset index\n",
    "    df = df.reset_index()\n",
    "    #set datetime featres\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "    df['fillingDate'] = pd.to_datetime(df['fillingDate']).dt.tz_localize(None)\n",
    "    df = df.set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    #create year and quarter based in fillingDate\n",
    "    df['quarter'] = pd.to_datetime(df.index.get_level_values(2)).quarter\n",
    "    df['year'] = pd.to_datetime(df.index.get_level_values(2)).year\n",
    "    \n",
    "    #generate target\n",
    "    df_target = getTarget(df.iloc[:,-5:],\n",
    "                          newTarget=newTarget)\n",
    "\n",
    "    #calculate nr of days between fillingDate and date to predict future returns\n",
    "    df = calculateDiffDays(df)\n",
    "\n",
    "    #drop leakage columns\n",
    "    to_drop = ['futDate',\n",
    "        'futstockPrice', 'futindexPrice', 'futAlpha']\n",
    "\n",
    "    df_clean = firstPreparation(df.drop(columns = to_drop))\n",
    "    \n",
    "    \n",
    "    nr_obs = df_clean.shape[0]\n",
    "    n_cols = len(df_clean.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    return(df_clean, df_target)\n",
    "\n",
    "\n",
    "#set color for graphs\n",
    "color1 = 'royalblue'\n",
    "\n",
    "\n",
    "input_path = r'E:\\Database\\Reasearch Topic\\Long-Term'\n",
    "data_path = r'fundamentalData_v2.csv'\n",
    "target_path = r'CAPM\\QuarterlyTarget_CAPM.csv'\n",
    "resample_path = r'QuarterlyPrices.csv'\n",
    "\n",
    "path = r'E:\\Database\\Reasearch Topic\\Long-Term\\fundamentalData.csv'\n",
    "\n",
    "\n",
    "df_clean, df_target = importData(cloud=False,\n",
    "                                 newTarget=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps:__  \n",
    "- Fix column names and eliminate duplicate columns  \n",
    "- Check data types and correct them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "reportedCurrency\n",
      "acceptedDate\n",
      "calendarYear\n",
      "period\n",
      "revenue\n",
      "costOfRevenue\n",
      "grossProfit\n",
      "grossProfitRatio\n",
      "researchAndDevelopmentExpenses\n",
      "generalAndAdministrativeExpenses\n",
      "sellingAndMarketingExpenses\n",
      "sellingGeneralAndAdministrativeExpenses\n",
      "otherExpenses\n",
      "operatingExpenses\n",
      "costAndExpenses\n",
      "interestIncome\n",
      "interestExpense\n",
      "depreciationAndAmortization\n",
      "ebitda\n",
      "ebitdaratio\n",
      "operatingIncome\n",
      "operatingIncomeRatio\n",
      "totalOtherIncomeExpensesNet\n",
      "incomeBeforeTax\n",
      "incomeBeforeTaxRatio\n",
      "incomeTaxExpense\n",
      "netIncome\n",
      "netIncomeRatio\n",
      "eps\n",
      "epsdiluted\n",
      "weightedAverageShsOut\n",
      "weightedAverageShsOutDil\n",
      "link\n",
      "finalLink\n",
      "cashAndCashEquivalents\n",
      "shortTermInvestments\n",
      "cashAndShortTermInvestments\n",
      "netReceivables\n",
      "inventory\n",
      "otherCurrentAssets\n",
      "totalCurrentAssets\n",
      "propertyPlantEquipmentNet\n",
      "goodwill\n",
      "intangibleAssets\n",
      "goodwillAndIntangibleAssets\n",
      "longTermInvestments\n",
      "taxAssets\n",
      "otherNonCurrentAssets\n",
      "totalNonCurrentAssets\n",
      "otherAssets\n",
      "totalAssets\n",
      "accountPayables\n",
      "shortTermDebt\n",
      "taxPayables\n",
      "deferredRevenue\n",
      "otherCurrentLiabilities\n",
      "totalCurrentLiabilities\n",
      "longTermDebt\n",
      "deferredRevenueNonCurrent\n",
      "deferredTaxLiabilitiesNonCurrent\n",
      "otherNonCurrentLiabilities\n",
      "totalNonCurrentLiabilities\n",
      "otherLiabilities\n",
      "capitalLeaseObligations\n",
      "totalLiabilities\n",
      "preferredStock\n",
      "commonStock\n",
      "retainedEarnings\n",
      "accumulatedOtherComprehensiveIncomeLoss\n",
      "othertotalStockholdersEquity\n",
      "totalStockholdersEquity\n",
      "totalEquity\n",
      "totalLiabilitiesAndStockholdersEquity\n",
      "minorityInterest\n",
      "totalLiabilitiesAndTotalEquity\n",
      "totalInvestments\n",
      "totalDebt\n",
      "netDebt\n",
      "netIncome_cf\n",
      "depreciationAndAmortization_cf\n",
      "deferredIncomeTax\n",
      "stockBasedCompensation\n",
      "changeInWorkingCapital\n",
      "accountsReceivables\n",
      "inventory_cf\n",
      "accountsPayables\n",
      "otherWorkingCapital\n",
      "otherNonCashItems\n",
      "netCashProvidedByOperatingActivities\n",
      "investmentsInPropertyPlantAndEquipment\n",
      "acquisitionsNet\n",
      "purchasesOfInvestments\n",
      "salesMaturitiesOfInvestments\n",
      "otherInvestingActivites\n",
      "netCashUsedForInvestingActivites\n",
      "debtRepayment\n",
      "commonStockIssued\n",
      "commonStockRepurchased\n",
      "dividendsPaid\n",
      "otherFinancingActivites\n",
      "netCashUsedProvidedByFinancingActivities\n",
      "effectOfForexChangesOnCash\n",
      "netChangeInCash\n",
      "cashAtEndOfPeriod\n",
      "cashAtBeginningOfPeriod\n",
      "operatingCashFlow\n",
      "capitalExpenditure\n",
      "freeCashFlow\n",
      "currentRatio\n",
      "quickRatio\n",
      "cashRatio\n",
      "daysOfSalesOutstanding\n",
      "daysOfInventoryOutstanding\n",
      "operatingCycle\n",
      "daysOfPayablesOutstanding\n",
      "cashConversionCycle\n",
      "grossProfitMargin\n",
      "operatingProfitMargin\n",
      "pretaxProfitMargin\n",
      "netProfitMargin\n",
      "effectiveTaxRate\n",
      "returnOnAssets\n",
      "returnOnEquity\n",
      "returnOnCapitalEmployed\n",
      "netIncomePerEBT\n",
      "ebtPerEbit\n",
      "ebitPerRevenue\n",
      "debtRatio\n",
      "debtEquityRatio\n",
      "longTermDebtToCapitalization\n",
      "totalDebtToCapitalization\n",
      "interestCoverage\n",
      "cashFlowToDebtRatio\n",
      "companyEquityMultiplier\n",
      "receivablesTurnover\n",
      "payablesTurnover\n",
      "inventoryTurnover\n",
      "fixedAssetTurnover\n",
      "assetTurnover\n",
      "operatingCashFlowPerShare\n",
      "freeCashFlowPerShare\n",
      "cashPerShare\n",
      "payoutRatio\n",
      "operatingCashFlowSalesRatio\n",
      "freeCashFlowOperatingCashFlowRatio\n",
      "cashFlowCoverageRatios\n",
      "shortTermCoverageRatios\n",
      "capitalExpenditureCoverageRatio\n",
      "dividendPaidAndCapexCoverageRatio\n",
      "dividendPayoutRatio\n",
      "priceBookValueRatio\n",
      "priceToBookRatio\n",
      "priceToSalesRatio\n",
      "priceEarningsRatio\n",
      "priceToFreeCashFlowsRatio\n",
      "priceToOperatingCashFlowsRatio\n",
      "priceCashFlowRatio\n",
      "priceEarningsToGrowthRatio\n",
      "priceSalesRatio\n",
      "dividendYield\n",
      "enterpriseValueMultiple\n",
      "priceFairValue\n",
      "quarter\n",
      "year\n",
      "floatShares\n",
      "dividend\n",
      "sector\n",
      "industry\n",
      "symbols\n",
      "Date_2\n",
      "QoQ_stockRetur\n",
      "close\n",
      "indexPrice\n",
      "beta\n",
      "alpha\n",
      "date_diff\n"
     ]
    }
   ],
   "source": [
    "df = df_clean.copy()\n",
    "print('Column Names:')\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Eliminate duplicate columns:__  \n",
    "During the pipeline that integrates multiple data sources in a single source, some columns that were repeated across sources were merged together. The goal is to identify those columns, eliminate the one that are duplicate and then rename the name of the others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>fillingDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AAL</th>\n",
       "      <th>2012-03-31</th>\n",
       "      <th>2012-04-19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-06-30</th>\n",
       "      <th>2003-06-30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <th>2018-07-26</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <th>2016-07-22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <th>2022-02-22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZUMZ</th>\n",
       "      <th>2010-07-31</th>\n",
       "      <th>2010-09-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-05</th>\n",
       "      <th>2007-06-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-28</th>\n",
       "      <th>2012-03-13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <th>2020-12-07</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-30</th>\n",
       "      <th>2022-06-06</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89936 rows Ã— 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(AAL, 2012-03-31 00:00:00, 2012-04-19 00:00:00), (AAL, 2003-06-30 00:00:00, 2003-06-30 00:00:00), (AAL, 2018-06-30 00:00:00, 2018-07-26 00:00:00), (AAL, 2016-06-30 00:00:00, 2016-07-22 00:00:00), (AAL, 2021-12-31 00:00:00, 2022-02-22 00:00:00), (AAL, 2021-03-31 00:00:00, 2021-04-22 00:00:00), (AAL, 2005-06-30 00:00:00, 2005-06-30 00:00:00), (AAL, 2015-09-30 00:00:00, 2015-10-23 00:00:00), (AAL, 2004-09-30 00:00:00, 2004-09-30 00:00:00), (AAL, 2004-12-31 00:00:00, 2005-02-25 00:00:00), (AAL, 2013-12-31 00:00:00, 2014-02-28 00:00:00), (AAL, 2006-09-30 00:00:00, 2006-10-20 00:00:00), (AAL, 2012-12-31 00:00:00, 2013-02-20 00:00:00), (AAL, 2018-12-31 00:00:00, 2019-02-25 00:00:00), (AAL, 2007-03-31 00:00:00, 2007-04-20 00:00:00), (AAL, 2014-03-31 00:00:00, 2014-04-24 00:00:00), (AAL, 2011-09-30 00:00:00, 2011-10-19 00:00:00), (AAL, 2011-12-31 00:00:00, 2012-02-15 00:00:00), (AAL, 2008-03-31 00:00:00, 2008-03-31 00:00:00), (AAL, 2016-03-31 00:00:00, 2016-04-22 00:00:00), (AAL, 2006-03-31 00:00:00, 2006-04-20 00:00:00), (AAL, 2020-03-31 00:00:00, 2020-04-30 00:00:00), (AAL, 2013-06-30 00:00:00, 2013-07-18 00:00:00), (AAL, 2019-12-31 00:00:00, 2020-02-19 00:00:00), (AAL, 2010-03-31 00:00:00, 2010-04-21 00:00:00), (AAL, 2011-03-31 00:00:00, 2011-04-20 00:00:00), (AAL, 2015-06-30 00:00:00, 2015-07-24 00:00:00), (AAL, 2017-09-30 00:00:00, 2017-10-26 00:00:00), (AAL, 2005-03-31 00:00:00, 2005-03-31 00:00:00), (AAL, 2012-09-30 00:00:00, 2012-10-17 00:00:00), (AAL, 2018-03-31 00:00:00, 2018-04-26 00:00:00), (AAL, 2009-03-31 00:00:00, 2009-03-31 00:00:00), (AAL, 2019-09-30 00:00:00, 2019-10-24 00:00:00), (AAL, 2017-03-31 00:00:00, 2017-04-27 00:00:00), (AAL, 2010-12-31 00:00:00, 2011-02-16 00:00:00), (AAL, 2016-09-30 00:00:00, 2016-10-20 00:00:00), (AAL, 2005-12-31 00:00:00, 2006-02-24 00:00:00), (AAL, 2022-12-31 00:00:00, 2023-02-22 00:00:00), (AAL, 2007-12-31 00:00:00, 2007-12-31 00:00:00), (AAL, 2010-06-30 00:00:00, 2010-07-21 00:00:00), (AAL, 2008-06-30 00:00:00, 2008-06-30 00:00:00), (AAL, 2022-06-30 00:00:00, 2022-07-21 00:00:00), (AAL, 2003-03-31 00:00:00, 2003-03-31 00:00:00), (AAL, 2019-06-30 00:00:00, 2019-07-25 00:00:00), (AAL, 2019-03-31 00:00:00, 2019-04-26 00:00:00), (AAL, 2018-09-30 00:00:00, 2018-10-25 00:00:00), (AAL, 2017-06-30 00:00:00, 2017-07-28 00:00:00), (AAL, 2009-09-30 00:00:00, 2009-09-30 00:00:00), (AAL, 2009-06-30 00:00:00, 2009-06-30 00:00:00), (AAL, 2014-12-31 00:00:00, 2015-02-25 00:00:00), (AAL, 2021-09-30 00:00:00, 2021-10-21 00:00:00), (AAL, 2013-09-30 00:00:00, 2013-10-17 00:00:00), (AAL, 2020-06-30 00:00:00, 2020-07-23 00:00:00), (AAL, 2010-09-30 00:00:00, 2010-10-20 00:00:00), (AAL, 2022-09-30 00:00:00, 2022-10-20 00:00:00), (AAL, 2003-12-31 00:00:00, 2004-02-27 00:00:00), (AAL, 2014-09-30 00:00:00, 2014-10-23 00:00:00), (AAL, 2004-03-31 00:00:00, 2004-04-23 00:00:00), (AAL, 2004-06-30 00:00:00, 2004-06-30 00:00:00), (AAL, 2013-03-31 00:00:00, 2013-04-18 00:00:00), (AAL, 2006-06-30 00:00:00, 2006-07-25 00:00:00), (AAL, 2009-12-31 00:00:00, 2010-02-17 00:00:00), (AAL, 2021-06-30 00:00:00, 2021-07-22 00:00:00), (AAL, 2017-12-31 00:00:00, 2018-02-21 00:00:00), (AAL, 2016-12-31 00:00:00, 2017-02-22 00:00:00), (AAL, 2015-03-31 00:00:00, 2015-04-24 00:00:00), (AAL, 2015-12-31 00:00:00, 2016-02-24 00:00:00), (AAL, 2005-09-30 00:00:00, 2005-09-30 00:00:00), (AAL, 2020-09-30 00:00:00, 2020-10-22 00:00:00), (AAL, 2014-06-30 00:00:00, 2014-07-24 00:00:00), (AAL, 2008-09-30 00:00:00, 2008-09-30 00:00:00), (AAL, 2022-03-31 00:00:00, 2022-04-21 00:00:00), (AAL, 2008-12-31 00:00:00, 2008-12-31 00:00:00), (AAL, 2012-06-30 00:00:00, 2012-07-18 00:00:00), (AAL, 2006-12-31 00:00:00, 2007-02-23 00:00:00), (AAL, 2003-09-30 00:00:00, 2003-09-30 00:00:00), (AAL, 2011-06-30 00:00:00, 2011-07-20 00:00:00), (AAL, 2020-12-31 00:00:00, 2021-02-17 00:00:00), (AAL, 2007-06-30 00:00:00, 2007-07-24 00:00:00), (AAON, 2021-03-31 00:00:00, 2021-05-06 00:00:00), (AAON, 2015-09-30 00:00:00, 2015-11-02 00:00:00), (AAON, 2020-12-31 00:00:00, 2021-02-25 00:00:00), (AAON, 2013-12-31 00:00:00, 2014-03-13 00:00:00), (AAON, 2011-03-31 00:00:00, 2011-05-05 00:00:00), (AAON, 2019-06-30 00:00:00, 2019-08-01 00:00:00), (AAON, 2004-06-30 00:00:00, 2004-08-09 00:00:00), (AAON, 2019-03-31 00:00:00, 2019-05-02 00:00:00), (AAON, 2004-09-30 00:00:00, 2004-11-12 00:00:00), (AAON, 2015-06-30 00:00:00, 2015-08-06 00:00:00), (AAON, 2014-12-31 00:00:00, 2015-02-27 00:00:00), (AAON, 2017-09-30 00:00:00, 2017-11-02 00:00:00), (AAON, 2010-03-31 00:00:00, 2010-05-06 00:00:00), (AAON, 2006-03-31 00:00:00, 2006-05-10 00:00:00), (AAON, 2007-12-31 00:00:00, 2008-03-12 00:00:00), (AAON, 2011-12-31 00:00:00, 2012-03-14 00:00:00), (AAON, 2013-06-30 00:00:00, 2013-08-08 00:00:00), (AAON, 2014-06-30 00:00:00, 2014-08-07 00:00:00), (AAON, 2003-06-30 00:00:00, 2003-08-13 00:00:00), (AAON, 2014-09-30 00:00:00, 2014-11-06 00:00:00), (AAON, 2013-09-30 00:00:00, 2013-11-07 00:00:00), ...]\n",
       "\n",
       "[89936 rows x 0 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get columns that have _x or _y or _z since this are probably duplicated columns\n",
    "duplicated_cols = df.columns[(df.columns.str.contains('_x'))| \n",
    "                            (df.columns.str.contains('_y'))|\n",
    "                            (df.columns.str.contains('_z'))]\n",
    "\n",
    "display(df[duplicated_cols])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check Data-types:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 89936 entries, ('AAL', Timestamp('2012-03-31 00:00:00'), Timestamp('2012-04-19 00:00:00')) to ('ZUMZ', Timestamp('2022-04-30 00:00:00'), Timestamp('2022-06-06 00:00:00'))\n",
      "Columns: 176 entries, reportedCurrency to date_diff\n",
      "dtypes: float64(166), object(10)\n",
      "memory usage: 121.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column reportedCurrency: object\n",
      "Column acceptedDate: object\n",
      "Column calendarYear: float64\n",
      "Column period: object\n",
      "Column revenue: float64\n",
      "Column costOfRevenue: float64\n",
      "Column grossProfit: float64\n",
      "Column grossProfitRatio: float64\n",
      "Column researchAndDevelopmentExpenses: float64\n",
      "Column generalAndAdministrativeExpenses: float64\n",
      "Column sellingAndMarketingExpenses: float64\n",
      "Column sellingGeneralAndAdministrativeExpenses: float64\n",
      "Column otherExpenses: float64\n",
      "Column operatingExpenses: float64\n",
      "Column costAndExpenses: float64\n",
      "Column interestIncome: float64\n",
      "Column interestExpense: float64\n",
      "Column depreciationAndAmortization: float64\n",
      "Column ebitda: float64\n",
      "Column ebitdaratio: float64\n",
      "Column operatingIncome: float64\n",
      "Column operatingIncomeRatio: float64\n",
      "Column totalOtherIncomeExpensesNet: float64\n",
      "Column incomeBeforeTax: float64\n",
      "Column incomeBeforeTaxRatio: float64\n",
      "Column incomeTaxExpense: float64\n",
      "Column netIncome: float64\n",
      "Column netIncomeRatio: float64\n",
      "Column eps: float64\n",
      "Column epsdiluted: float64\n",
      "Column weightedAverageShsOut: float64\n",
      "Column weightedAverageShsOutDil: float64\n",
      "Column link: object\n",
      "Column finalLink: object\n",
      "Column cashAndCashEquivalents: float64\n",
      "Column shortTermInvestments: float64\n",
      "Column cashAndShortTermInvestments: float64\n",
      "Column netReceivables: float64\n",
      "Column inventory: float64\n",
      "Column otherCurrentAssets: float64\n",
      "Column totalCurrentAssets: float64\n",
      "Column propertyPlantEquipmentNet: float64\n",
      "Column goodwill: float64\n",
      "Column intangibleAssets: float64\n",
      "Column goodwillAndIntangibleAssets: float64\n",
      "Column longTermInvestments: float64\n",
      "Column taxAssets: float64\n",
      "Column otherNonCurrentAssets: float64\n",
      "Column totalNonCurrentAssets: float64\n",
      "Column otherAssets: float64\n",
      "Column totalAssets: float64\n",
      "Column accountPayables: float64\n",
      "Column shortTermDebt: float64\n",
      "Column taxPayables: float64\n",
      "Column deferredRevenue: float64\n",
      "Column otherCurrentLiabilities: float64\n",
      "Column totalCurrentLiabilities: float64\n",
      "Column longTermDebt: float64\n",
      "Column deferredRevenueNonCurrent: float64\n",
      "Column deferredTaxLiabilitiesNonCurrent: float64\n",
      "Column otherNonCurrentLiabilities: float64\n",
      "Column totalNonCurrentLiabilities: float64\n",
      "Column otherLiabilities: float64\n",
      "Column capitalLeaseObligations: float64\n",
      "Column totalLiabilities: float64\n",
      "Column preferredStock: float64\n",
      "Column commonStock: float64\n",
      "Column retainedEarnings: float64\n",
      "Column accumulatedOtherComprehensiveIncomeLoss: float64\n",
      "Column othertotalStockholdersEquity: float64\n",
      "Column totalStockholdersEquity: float64\n",
      "Column totalEquity: float64\n",
      "Column totalLiabilitiesAndStockholdersEquity: float64\n",
      "Column minorityInterest: float64\n",
      "Column totalLiabilitiesAndTotalEquity: float64\n",
      "Column totalInvestments: float64\n",
      "Column totalDebt: float64\n",
      "Column netDebt: float64\n",
      "Column netIncome_cf: float64\n",
      "Column depreciationAndAmortization_cf: float64\n",
      "Column deferredIncomeTax: float64\n",
      "Column stockBasedCompensation: float64\n",
      "Column changeInWorkingCapital: float64\n",
      "Column accountsReceivables: float64\n",
      "Column inventory_cf: float64\n",
      "Column accountsPayables: float64\n",
      "Column otherWorkingCapital: float64\n",
      "Column otherNonCashItems: float64\n",
      "Column netCashProvidedByOperatingActivities: float64\n",
      "Column investmentsInPropertyPlantAndEquipment: float64\n",
      "Column acquisitionsNet: float64\n",
      "Column purchasesOfInvestments: float64\n",
      "Column salesMaturitiesOfInvestments: float64\n",
      "Column otherInvestingActivites: float64\n",
      "Column netCashUsedForInvestingActivites: float64\n",
      "Column debtRepayment: float64\n",
      "Column commonStockIssued: float64\n",
      "Column commonStockRepurchased: float64\n",
      "Column dividendsPaid: float64\n",
      "Column otherFinancingActivites: float64\n",
      "Column netCashUsedProvidedByFinancingActivities: float64\n",
      "Column effectOfForexChangesOnCash: float64\n",
      "Column netChangeInCash: float64\n",
      "Column cashAtEndOfPeriod: float64\n",
      "Column cashAtBeginningOfPeriod: float64\n",
      "Column operatingCashFlow: float64\n",
      "Column capitalExpenditure: float64\n",
      "Column freeCashFlow: float64\n",
      "Column currentRatio: float64\n",
      "Column quickRatio: float64\n",
      "Column cashRatio: float64\n",
      "Column daysOfSalesOutstanding: float64\n",
      "Column daysOfInventoryOutstanding: float64\n",
      "Column operatingCycle: float64\n",
      "Column daysOfPayablesOutstanding: float64\n",
      "Column cashConversionCycle: float64\n",
      "Column grossProfitMargin: float64\n",
      "Column operatingProfitMargin: float64\n",
      "Column pretaxProfitMargin: float64\n",
      "Column netProfitMargin: float64\n",
      "Column effectiveTaxRate: float64\n",
      "Column returnOnAssets: float64\n",
      "Column returnOnEquity: float64\n",
      "Column returnOnCapitalEmployed: float64\n",
      "Column netIncomePerEBT: float64\n",
      "Column ebtPerEbit: float64\n",
      "Column ebitPerRevenue: float64\n",
      "Column debtRatio: float64\n",
      "Column debtEquityRatio: float64\n",
      "Column longTermDebtToCapitalization: float64\n",
      "Column totalDebtToCapitalization: float64\n",
      "Column interestCoverage: float64\n",
      "Column cashFlowToDebtRatio: float64\n",
      "Column companyEquityMultiplier: float64\n",
      "Column receivablesTurnover: float64\n",
      "Column payablesTurnover: float64\n",
      "Column inventoryTurnover: float64\n",
      "Column fixedAssetTurnover: float64\n",
      "Column assetTurnover: float64\n",
      "Column operatingCashFlowPerShare: float64\n",
      "Column freeCashFlowPerShare: float64\n",
      "Column cashPerShare: float64\n",
      "Column payoutRatio: float64\n",
      "Column operatingCashFlowSalesRatio: float64\n",
      "Column freeCashFlowOperatingCashFlowRatio: float64\n",
      "Column cashFlowCoverageRatios: float64\n",
      "Column shortTermCoverageRatios: float64\n",
      "Column capitalExpenditureCoverageRatio: float64\n",
      "Column dividendPaidAndCapexCoverageRatio: float64\n",
      "Column dividendPayoutRatio: float64\n",
      "Column priceBookValueRatio: float64\n",
      "Column priceToBookRatio: float64\n",
      "Column priceToSalesRatio: float64\n",
      "Column priceEarningsRatio: float64\n",
      "Column priceToFreeCashFlowsRatio: float64\n",
      "Column priceToOperatingCashFlowsRatio: float64\n",
      "Column priceCashFlowRatio: float64\n",
      "Column priceEarningsToGrowthRatio: float64\n",
      "Column priceSalesRatio: float64\n",
      "Column dividendYield: float64\n",
      "Column enterpriseValueMultiple: float64\n",
      "Column priceFairValue: float64\n",
      "Column quarter: float64\n",
      "Column year: float64\n",
      "Column floatShares: float64\n",
      "Column dividend: float64\n",
      "Column sector: object\n",
      "Column industry: object\n",
      "Column symbols: object\n",
      "Column Date_2: object\n",
      "Column QoQ_stockRetur: float64\n",
      "Column close: float64\n",
      "Column indexPrice: float64\n",
      "Column beta: float64\n",
      "Column alpha: float64\n",
      "Column date_diff: object\n"
     ]
    }
   ],
   "source": [
    "for col , type_ in zip(df_clean.dtypes.index, df_clean.dtypes):\n",
    "    print(f'Column {col}: {type_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of observations : 89936\n",
      "Nr of columns : 176\n"
     ]
    }
   ],
   "source": [
    "nr_obs = df.shape[0]\n",
    "n_cols = len(df_clean.columns)\n",
    "print(f'Nr of observations : {nr_obs}')\n",
    "print(f'Nr of columns : {n_cols}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import technical indicators from local disk\n",
      "Nr of observations : 115699\n",
      "Nr of columns : 223\n"
     ]
    }
   ],
   "source": [
    "def genGrowthV2 (df, growth_cols = None):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\n",
    "    Receives:\n",
    "        pandas df with quarterly data \n",
    "        growth_cols - columns where growth will be calculated\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    #get growth columns\n",
    "    if growth_cols == None:\n",
    "        growth_cols = ['revenue', 'costOfRevenue',\n",
    "       'grossProfit', 'researchAndDevelopmentExpenses', 'costAndExpenses',  'ebitda', 'operatingIncome',\n",
    "            'netIncome', 'eps','quarter','year']\n",
    "\n",
    "    #filter data to get only growth columns\n",
    "    df_temp = df[growth_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "\n",
    "    dates_temp = df_temp.reset_index().set_index(['symbol','year','quarter']).unstack('symbol')[['date','fillingDate']]\n",
    "    date = dates_temp.stack('symbol')\n",
    "\n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "    #calculate QoQ growth\n",
    "    QoQ_growth = growth_df.pct_change()\n",
    "    QoQ_growth.replace([np.inf], 1, inplace=True)\n",
    "    QoQ_growth.replace([-np.inf], -1, inplace=True)\n",
    "    QoQ_growth = QoQ_growth.iloc[1:]\n",
    "\n",
    "    QoQ_growth = QoQ_growth.fillna( QoQ_growth.median())\n",
    "    QoQ_growth = QoQ_growth.stack('symbol').fillna(0)\n",
    "\n",
    "    cols = QoQ_growth.columns\n",
    "    for col in cols:\n",
    "            QoQ_growth.rename(columns = {col:f'QoQ_{col}'},\n",
    "                             inplace=True)\n",
    "            \n",
    "    #calculate YoY growth\n",
    "    YoY_growth = growth_df.pct_change(4)\n",
    "    YoY_growth.replace([np.inf], 0, inplace=True)\n",
    "    YoY_growth.replace([-np.inf], 0, inplace=True)\n",
    "    YoY_growth = YoY_growth.iloc[4:]\n",
    "\n",
    "    YoY_growth = YoY_growth.stack('symbol')\n",
    "    YoY_growth = YoY_growth.unstack('symbol')\n",
    "    YoY_growth = YoY_growth.fillna( YoY_growth.median())\n",
    "    YoY_growth = YoY_growth.stack('symbol').fillna(0)\n",
    "\n",
    "\n",
    "    for col in cols:\n",
    "            YoY_growth.rename(columns = {col:f'YoY_{col}'},\n",
    "                             inplace=True)\n",
    "\n",
    "    #merge data\n",
    "    YoY_growth = pd.merge(YoY_growth,\n",
    "                          date.reset_index().rename(columns = {0:'date'}),\n",
    "            on = ['symbol','year','quarter'],\n",
    "            how='left').set_index(['symbol','year','quarter'])\n",
    "\n",
    "\n",
    "    full_growth = pd.merge(YoY_growth,\n",
    "            QoQ_growth,\n",
    "            how = 'left',\n",
    "            on = ['symbol','quarter','year'])\n",
    "    \n",
    "    #remove columns where growth can't be calculated \n",
    "    full_growth = full_growth.loc[full_growth['date'].isnull()==False]\n",
    "    return(full_growth, growth_cols)\n",
    "\n",
    "def gen_TTMV2 (df, ttm_cols = None):\n",
    "    \n",
    "    if ttm_cols == None:\n",
    "        ttm_cols = ['revenue', 'costOfRevenue','grossProfit',\n",
    "                'researchAndDevelopmentExpenses', 'costAndExpenses', 'ebitda', 'operatingIncome',\n",
    "                'netIncome', 'eps']\n",
    "\n",
    "    df_temp = df[ttm_cols].copy()\n",
    "    df_temp['quarter'] =pd.to_datetime(df_temp.reset_index()['date']).dt.quarter.values\n",
    "    df_temp['year'] =pd.to_datetime(df_temp.reset_index()['date']).dt.year.values\n",
    "\n",
    "    dates_temp = df_temp.reset_index().set_index(['symbol','year','quarter']).unstack('symbol')[['date','fillingDate']]\n",
    "    date = dates_temp.stack('symbol')\n",
    "\n",
    "    growth_df = df_temp.reset_index().set_index(['symbol','year','quarter']).drop(columns = ['fillingDate','date']).unstack('symbol')\n",
    "\n",
    "\n",
    "    ttm_df = growth_df.rolling(4).sum()\n",
    "    ttm_df = ttm_df.iloc[4:]\n",
    "    ttm_df = ttm_df.stack('symbol')\n",
    "    \n",
    "    cols = ttm_df.columns\n",
    "    for col in cols:\n",
    "            ttm_df.rename(columns = {col:f'{col}_TTM'},\n",
    "                             inplace=True)\n",
    "            \n",
    "    ttm_df = pd.merge(ttm_df,\n",
    "                    date.reset_index().rename(columns = {0:'date'}),\n",
    "            on = ['symbol','year','quarter'],\n",
    "            how='left').set_index(['symbol','year','quarter'])\n",
    "            \n",
    "    return(ttm_df)\n",
    "\n",
    "\n",
    "def fund_df_engineer_0 (fund_df):\n",
    "    #calculate ebit and earnings\n",
    "    fund_df['ebit'] = fund_df['ebitda'] - fund_df['depreciationAndAmortization']\n",
    "    fund_df['earnings'] =fund_df['ebit'] -  (fund_df['interestExpense'] + fund_df['incomeTaxExpense'])\n",
    "    fund_df['totalDividend'] = fund_df['dividend'] * fund_df['weightedAverageShsOut']\n",
    "    return(fund_df)\n",
    "\n",
    "\n",
    "def mergeValue (full_eng):\n",
    "    project_id = 'stockmarket-v0'\n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `stockmarket-v0.stockMarket_dev.enterpriseValue` as df\n",
    "\n",
    "    \"\"\"\n",
    "    df_value = pd.read_gbq(query = query, project_id = project_id)\n",
    "    df_value['date'] = pd.to_datetime(df_value['date'])\n",
    "    full_eng = pd.merge(full_eng.reset_index(),\n",
    "                        df_value,\n",
    "                        on = ['symbol','date'],\n",
    "                        how='left')\n",
    "    return(full_eng)\n",
    "\n",
    "\n",
    "def importIndicators (cloud = False):\n",
    "    #import indicators from bigquery\n",
    "    if cloud ==True:\n",
    "        print('Import technical indicators from cloud')\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `stockmarket-v0.stockMarket_dev.QuarterlyTechnicalIndicators`as df\n",
    "        \"\"\"\n",
    "        projectID = 'stockmarket-v0'\n",
    "        df_indicators = pd.read_gbq(query = query, project_id= projectID)\n",
    "    #import indicators from csv (local machine) \n",
    "    if cloud ==False:\n",
    "        print('Import technical indicators from local disk')\n",
    "        path = r'D:\\Desktop\\Long-Term-Model\\Data\\data_Technicalindicators.csv'\n",
    "        df_indicators = pd.read_csv(path).drop(columns = 'Unnamed: 0')\n",
    "    \n",
    "    df_indicators = df_indicators.loc[df_indicators.MA_21.isnull()==False]\n",
    "    df_indicators['fillingDate'] = pd.to_datetime(df_indicators['fillingDate'])\n",
    "    \n",
    "    return(df_indicators)\n",
    "\n",
    "\n",
    "def eliminateGaps (data):\n",
    "    data['lastFilling'] = data.reset_index().groupby('symbol').shift(1)['fillingDate'].values\n",
    "    data['fillingDiff'] = pd.to_datetime(data['fillingDate']) - pd.to_datetime(data['lastFilling'])\n",
    "    \n",
    "    data = data.loc[data['fillingDiff']<'200 days']\n",
    "    return(data)\n",
    "\n",
    "def replace_inf_with_zero(df):\n",
    "    # Replace infinite values with 0\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def full_FT_Eng (df_clean):\n",
    "    df_eng = df_clean.copy()\n",
    "    \n",
    "    #create columns related to financial ratios\n",
    "    df_eng = fund_df_engineer_0(df_eng)\n",
    "\n",
    "    ttm_cols = ['revenue', 'costOfRevenue','grossProfit',\n",
    "                'researchAndDevelopmentExpenses', 'costAndExpenses', 'ebitda', 'operatingIncome',\n",
    "                'netIncome', 'eps','earnings']\n",
    "\n",
    "    #calculate TTM and growth columns\n",
    "    df_ttm = gen_TTMV2(df_eng,ttm_cols)\n",
    "    df_growth, growth_cols = genGrowthV2(df_eng,)\n",
    "\n",
    "    #create a copy of DF\n",
    "    df_eng_ = df_eng\n",
    "\n",
    "    #sort by symbol and filling date\n",
    "    df_eng_2 = df_eng_.sort_values(['symbol','fillingDate'])\n",
    "\n",
    "\n",
    "    #match indexes of the created datasets with the main df\n",
    "    df_ttm = df_ttm.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    df_growth = df_growth.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    df_eng_2 = df_eng_2.reset_index().set_index(['symbol','fillingDate','date'])\n",
    "    \n",
    "    #filter index of main data to account for lagging features\n",
    "    df_eng_2 = df_eng_2.loc[df_eng_2.index.isin(df_ttm.index)]\n",
    "\n",
    "\n",
    "    #merge main data with ttm \n",
    "    full_eng = pd.merge(df_eng_2, df_ttm.drop(columns = ['year','quarter']),\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how = 'right')\n",
    "    \n",
    "    #merge main data with growth \n",
    "    full_eng = pd.merge(full_eng, df_growth.drop(columns = ['year','quarter']),\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how = 'left')\n",
    "    \n",
    "    full_eng = mergeValue(full_eng).set_index(['symbol','fillingDate','date'])\n",
    "    \n",
    "    \n",
    "    #imort technical indicators\n",
    "    df_indicators = importIndicators(cloud=False)\n",
    "    if 'date' in df_indicators.columns:\n",
    "        df_indicators = df_indicators.drop(columns = 'date')\n",
    "    #merge fundamental with technical indicators \n",
    "    full_eng = pd.merge(full_eng.reset_index(),\n",
    "                        df_indicators,\n",
    "                        on=['symbol','fillingDate'],\n",
    "                        how='left').set_index(['symbol','date','fillingDate','year','quarter'])\n",
    "    \n",
    "    full_eng = replace_inf_with_zero(full_eng)\n",
    "    \n",
    "    \n",
    "    \n",
    "    nr_obs = full_eng.shape[0]\n",
    "    n_cols = len(full_eng.columns)\n",
    "    print(f'Nr of observations : {nr_obs}')\n",
    "    print(f'Nr of columns : {n_cols}')\n",
    "    \n",
    "    return(full_eng)\n",
    "\n",
    "full_eng = full_FT_Eng(df_clean)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop observations with missmatched Financial Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of observations : 86516\n",
      "Nr of columns : 223\n"
     ]
    }
   ],
   "source": [
    "#drop tickers where financial ratios are missmatched\n",
    "tickers_toDrop = full_eng.loc[full_eng['priceToSalesRatio'].isnull()==True].reset_index()['symbol'].unique()\n",
    "full_eng = full_eng.loc[full_eng.index.get_level_values(0).isin(tickers_toDrop)==False]\n",
    "\n",
    "nr_obs = full_eng.shape[0]\n",
    "n_cols = len(full_eng.columns)\n",
    "print(f'Nr of observations : {nr_obs}')\n",
    "print(f'Nr of columns : {n_cols}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this stage of the process was to fill missing values in columns related to dividends with 0 since its reasonable to assume that some companies don't pay dividends, after filling those values, remaining columns that have more than 30% missing data were removed. These features were removed because imputing a certain value would introduce a lot of bias that could negatively impact the model.v Only one columns fell into the requirements, namely 'YoY_researchAndDevelopmentExpenses' that had 82% of the data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with most missing values:\n",
      "dividend               40940\n",
      "totalDividend          40940\n",
      "floatShares            34848\n",
      "operatingCycle         26999\n",
      "cashConversionCycle    26999\n",
      "MA_42                  11544\n",
      "MA_21                  11544\n",
      "Volatility_21          11544\n",
      "excessReturn_21        11544\n",
      "excessReturn_84        11544\n",
      "Volatility_42          11544\n",
      "MA_63                  11544\n",
      "excessReturn_42        11544\n",
      "Volatility_84          11544\n",
      "Volatility_63          11544\n",
      "dtype: int64\n",
      " \n",
      "Features with most missing values, in %:\n",
      " \n",
      "dividend               0.473207\n",
      "totalDividend          0.473207\n",
      "floatShares            0.402793\n",
      "operatingCycle         0.312069\n",
      "cashConversionCycle    0.312069\n",
      "MA_42                  0.133432\n",
      "MA_21                  0.133432\n",
      "Volatility_21          0.133432\n",
      "excessReturn_21        0.133432\n",
      "excessReturn_84        0.133432\n",
      "Volatility_42          0.133432\n",
      "MA_63                  0.133432\n",
      "excessReturn_42        0.133432\n",
      "Volatility_84          0.133432\n",
      "Volatility_63          0.133432\n",
      "dtype: float64\n",
      "3 features were removed beucause they has a more than 30.0% of observations missing.\n",
      "Features removed: ['floatShares', 'cashConversionCycle', 'operatingCycle']\n",
      "Imputing missing data with interactive imputer- Decision Tree is base model\n",
      "Nr of missing values in numerical features after interactive imputer: 0\n",
      "Nr of missing values in categorical features after interactive imputer: 44606\n",
      "Features with missing values ['Date_2', 'symbols', 'finalLink', 'link', 'acceptedDate']\n"
     ]
    }
   ],
   "source": [
    "def remove_cols_missing (x_missing, thresh):\n",
    "    '''''''''''''''\n",
    "    receives:\n",
    "        DF\n",
    "        thresh - % threshold\n",
    "    Removes all columns that have more than a certain % missing\n",
    "        \n",
    "    '''''''''''''''\n",
    "\n",
    "    mv_ = (x_missing.isnull().sum() / len(x_missing)).sort_values(ascending=False)\n",
    "    deleted_ = []\n",
    "    for col, value in zip(mv_.index, mv_.values):\n",
    "        if value>=thresh:\n",
    "            x_missing = x_missing.drop(columns=f'{col}')\n",
    "            deleted_.append(col)\n",
    "\n",
    "    print(f'{len(deleted_)} features were removed beucause they has a more than {thresh*100}% of observations missing.')\n",
    "    print(f'Features removed: {deleted_}')\n",
    "    return(x_missing)\n",
    "\n",
    "def interactiveImputer (data, estimator):\n",
    "    from sklearn.experimental import enable_iterative_imputer  \n",
    "    from sklearn.impute import  SimpleImputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    metric_features = data.select_dtypes(include=np.number).columns\n",
    "    cat_features = data.select_dtypes(exclude=np.number).columns\n",
    "    \n",
    "    if len(data)<1000:\n",
    "        sample_size = len(data)\n",
    "    else:\n",
    "        sample_size = 1000\n",
    "    \n",
    "    imputer = IterativeImputer(estimator=estimator)\n",
    "    imputer.fit(data.sample(sample_size)[metric_features])\n",
    "    data_imputed = imputer.transform(data[metric_features])\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=metric_features, index = data.index)\n",
    "    return(pd.concat([data_imputed,data[cat_features]],axis=1), imputer)\n",
    "\n",
    "def missingValues (df_missing):\n",
    "    #get a copy of the df\n",
    "    #fill dividend missing values with 0 \n",
    "    dividend_cols = df_missing.columns[(df_missing.columns.str.contains('dividend'))  \\\n",
    "                                       | (df_missing.columns.str.contains('Dividend'))]\n",
    "    \n",
    "    #for each column in dividends, fill missing values with 0\n",
    "    for col in dividend_cols:\n",
    "        df_missing[col] = df_missing[col].fillna(0)\n",
    "\n",
    "    #for every columns more than  30% of the data missing\n",
    "    thresh = 0.30\n",
    "    df_missing = remove_cols_missing(df_missing.copy(),thresh)\n",
    "    \n",
    "    #get numerical, categoricsal and boolean columns\n",
    "    num_cols = df_missing.select_dtypes(include = np.number).columns\n",
    "    cat_cols = df_missing.select_dtypes(exclude = np.number).columns\n",
    "    bool_cols = df_missing.select_dtypes(include = np.bool).columns\n",
    "    \n",
    "    return(df_missing, num_cols,cat_cols,bool_cols)\n",
    "\n",
    "def imputMissingValues (df):\n",
    "    print('Features with most missing values:')\n",
    "    print(full_eng.isnull().sum().sort_values(ascending = False).iloc[:15])\n",
    "    print(' ')\n",
    "    print('Features with most missing values, in %:')\n",
    "    print(' ')\n",
    "    print((full_eng.isnull().sum()/len(full_eng)).sort_values(ascending = False).iloc[:15])\n",
    "    df_missing, num_cols,cat_cols,bool_cols = missingValues(df)\n",
    "    \n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "    num_cols = df_missing.select_dtypes(include = np.number).columns\n",
    "    cat_cols = df_missing.select_dtypes(include = np.bool_).columns\n",
    "    bool_cols = df_missing.select_dtypes(include = np.object).columns\n",
    "\n",
    "    model = DecisionTreeRegressor(random_state=0,\n",
    "                                  max_depth=3)\n",
    "    \n",
    "    print('Imputing missing data with interactive imputer- Decision Tree is base model')\n",
    "    #imput missing values in numerical features using interactive imputer with decision tree\n",
    "    df_imput_dt, imputer = interactiveImputer(df_missing[num_cols],\n",
    "                                                 model)\n",
    "\n",
    "    #concat df with num features with df with bool and categorical features\n",
    "    df_imput = pd.concat([df_imput_dt, \n",
    "                          df_missing[bool_cols]],\n",
    "                         axis=1)\n",
    "    df_imput = pd.concat([df_imput,\n",
    "                          df_missing[cat_cols]],\n",
    "                         axis=1)\n",
    "    \n",
    "#     fill missing values in sector and industry with unkown\n",
    "    df_imput.loc[df_imput['sector'].isnull()==True,\n",
    "                 'sector'] = 'Unkown'\n",
    "    df_imput.loc[df_imput['industry'].isnull()==True,\n",
    "                 'industry'] = 'Unkown'\n",
    "    \n",
    "    mv_cat = df_imput.isnull().sum().sort_values(ascending=False).iloc[:10]\n",
    "    mv_cat = mv_cat[mv_cat.values>0].index\n",
    "\n",
    "    mv_ = df_imput.select_dtypes(include = np.number).isnull().sum().sum()\n",
    "    print(f'Nr of missing values in numerical features after interactive imputer: {mv_}')\n",
    "    mv_ = df_imput.select_dtypes(exclude = np.number).isnull().sum().sum()\n",
    "    print(f'Nr of missing values in categorical features after interactive imputer: {mv_}')\n",
    "\n",
    "    print(f'Features with missing values {list(mv_cat)}')\n",
    "    \n",
    "    return(df_imput)\n",
    "\n",
    "df_imput = imputMissingValues(full_eng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer - After Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of rows:\n",
      "73551\n",
      "99.0% of the data remained after pre-processing\n",
      "Nr of observations : 72712\n",
      "Nr of columns : 382\n"
     ]
    }
   ],
   "source": [
    "def gen_DFModel (df_imput,df_target):\n",
    "    #drop duplicates and set new index\n",
    "    df_full = df_imput.reset_index().drop_duplicates(['symbol','date']).set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    #drop duplicates and set new index\n",
    "    df_target = df_target.reset_index().drop_duplicates(['symbol','date']).set_index(['symbol','date','fillingDate'])\n",
    "\n",
    "    df_model = pd.merge(df_full,\n",
    "                        df_target.iloc[:,:],\n",
    "                        on = ['symbol','date','fillingDate'],\n",
    "                        how='left')\n",
    "\n",
    "    #drop observations where the target is null \n",
    "    #might be companies where daily price isn't available \n",
    "    #might be targets can't be calculated \n",
    "    df_model = df_model.loc[(df_model['Target_0.05'].isnull()==False)&\n",
    "                           (df_model['excessReturns'].isnull()==False)]\n",
    "    \n",
    "    df_model['date_diff'] = df_model['date_diff'].astype(int)\n",
    "    \n",
    "    return(df_model)\n",
    "\n",
    "#incoherence checking\n",
    "def perform_Incoherence (data):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Receives:\n",
    "        data - pandas df after being merged with Target\n",
    "        \n",
    "    Removes incoherences from the dataset \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    df_inc = data.copy()\n",
    "    #set incoherences\n",
    "    df_inc = df_inc.loc[df_inc['totalDebt']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['totalAssets']>0]\n",
    "    df_inc = df_inc.loc[df_inc['revenue']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['weightedAverageShsOut']>=0]\n",
    "    df_inc = df_inc.loc[df_inc['date_diff']<100]\n",
    "\n",
    "    df_inc_ = data.loc[data.index.isin(df_inc.index)]\n",
    "    init_len = len(data)\n",
    "    pct_rem = np.round(len(df_inc_) / init_len, 3)\n",
    "\n",
    "    print(f'{pct_rem*100}% of the data remained after pre-processing')\n",
    "    \n",
    "    return(df_inc_)\n",
    "\n",
    "\n",
    "def get_Binaries(df):\n",
    "    \n",
    "    col_name = 'priceEarningsRatio'\n",
    "    new_col = 'sector_priceEarningsRatio_bin'\n",
    "    for sector in df['sector'].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df['sector']==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[ cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[ cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    \n",
    "    col_name = 'netDebt'\n",
    "    new_col = 'netDebt_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    \n",
    "    col_name = 'netIncome_TTM'\n",
    "    new_col = 'netIncome_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.7)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    \n",
    "    col_name = 'netProfitMargin'\n",
    "    new_col = 'netProfitMargin_bin'\n",
    "    division = 'sector'\n",
    "    for sector in df[division].unique():\n",
    "        for quarter in df.reset_index()['quarter'].unique():\n",
    "            for year in df.reset_index()['year'].unique():\n",
    "                cond_industry  = (df[division]==f'{sector}')\n",
    "                cond_quarter = (df.index.get_level_values(4)==quarter)\n",
    "                cond_year = (df.index.get_level_values(3)==year)\n",
    "                val = df.loc[cond_quarter & cond_year, col_name].quantile(0.2)\n",
    "                cond_value =  (df[col_name]<val)\n",
    "                df.loc[  cond_quarter & cond_year & \\\n",
    "                       cond_value   ,new_col] = 1\n",
    "\n",
    "\n",
    "\n",
    "    df[new_col] = df[new_col].fillna(0).astype(bool)\n",
    "    sectors = ['Services', 'Technology', 'Basic Materials', 'Consumer Cyclical']\n",
    "    \n",
    "    df.loc[df['sector'].isin(sectors),'mainSector'] = 1\n",
    "    df['mainSector'] = df['mainSector'].fillna(0).astype(bool)\n",
    "    return(df)\n",
    "\n",
    "def gen_categorical (df):\n",
    "    '''''''''''\n",
    "    \n",
    "    receives \n",
    "        pandas df\n",
    "    \n",
    "    Generates Marketcap categories based on it's values\n",
    "    \n",
    "    '''''''''\n",
    "    df['mktcap'] = (df['close'] * df['weightedAverageShsOut'])\n",
    "    \n",
    "    #create marketcap scales\n",
    "    bil = (1000000000)\n",
    "\n",
    "    #if mktcap > 50 billions then mega\n",
    "    df.loc[df['mktcap']>=bil*50,'marketcap'] = 'Mega'\n",
    "\n",
    "    #if mktcap  10 < mktcap <50 billion then large\n",
    "    df.loc[(df['mktcap']>=bil*10) & (df['mktcap']<bil*50),'marketcap'] = 'Large'\n",
    "\n",
    "    #if mktcap  2 < mktcap <10 billion then mid\n",
    "    df.loc[(df['mktcap']<bil*10) & (df['mktcap']>=bil*2),'marketcap'] = 'Mid'\n",
    "\n",
    "    #if mktcap   mktcap <2 billion then small\n",
    "    df.loc[(df['mktcap']<bil*2) ,'marketcap'] = 'Small'\n",
    "\n",
    "    #if mktcap   mktcap <300 millions then mucro\n",
    "    df.loc[(df['mktcap']<300) ,'marketcap'] = 'Micro'\n",
    "\n",
    "    mkcap_list = ['Small','Mid']\n",
    "    df.loc[df['marketcap'].isin(mkcap_list),'marketcap_bin'] = 1\n",
    "    df['marketcap_bin'] = df['marketcap_bin'].fillna(0)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def oneHotEncoder (data):\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "    non_metric_features =list(data.select_dtypes(exclude=np.number).set_index(data.index).columns)\n",
    "    ohc = OneHotEncoder(sparse=False)\n",
    "    ohc_feat = ohc.fit_transform(data[non_metric_features])\n",
    "\n",
    "    names = ohc.get_feature_names_out()\n",
    "    \n",
    "    ohc_cat = pd.DataFrame(data =ohc_feat ,columns = names, index = data.index)\n",
    "    return(ohc_cat)\n",
    "\n",
    "\n",
    "def analyzeQuantiles (data,):\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    Receives:\n",
    "            pandas dataframe\n",
    "            \n",
    "    Creates a df that analyzes outliers by creating 2 disparity features that measure the level of the outliers.\n",
    "    The fist measure divides de 95% quantile by the 90% and the second divides the mean by the median\n",
    "    \n",
    "    Note: thresholds can be further adjusted\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df_out = data\n",
    "    \n",
    "    quantile_disp = np.round(df_out.describe([.25, .5, .75,0.9,0.95]).T).iloc[:,:-1]\n",
    "    quantile_disp['disparity'] = (quantile_disp['95%'] / quantile_disp['90%']).replace(np.inf,0)\n",
    "    quantile_disp['mean_disp'] = (quantile_disp['mean'] / quantile_disp['50%']).replace(np.inf,0)\n",
    "\n",
    "    quantile_disp = quantile_disp.sort_values(ascending=False,by = 'mean_disp')\n",
    "    display(quantile_disp.iloc[:10])\n",
    "    \n",
    "    return(quantile_disp)\n",
    "\n",
    "\n",
    "#replace outliers with quantie \n",
    "def replaceOutliers (data,upper_thresh, lower_thresh):\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    Receives:\n",
    "            pandas dataframe\n",
    "            \n",
    "    Replaces outliers with the nearest quantile, per example an observation of quantile 99 will be replaced by the quantile 90\n",
    "    \n",
    "    Note: thresholds can be fyrther adjusted\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    df_out = data.copy()\n",
    "    \n",
    "    cols = df_out.select_dtypes(include = np.number).columns\n",
    "    rem_cols = df_out.select_dtypes(exclude = np.number).columns\n",
    "    for col in cols:\n",
    "        \n",
    "        upper_quant = df_out[col].quantile(upper_thresh)\n",
    "        lower_quant = df_out[col].quantile(lower_thresh)\n",
    "        \n",
    "        df_out.loc[df_out[col]>upper_quant, col] = upper_quant\n",
    "        df_out.loc[df_out[col]<lower_quant, col] = lower_quant\n",
    "        \n",
    "        \n",
    "    df_out = pd.concat([df_out[cols],\n",
    "                        df_out[rem_cols]],\n",
    "                      axis=1)\n",
    "        \n",
    "    return(df_out)\n",
    "\n",
    "#apply log an cubic root transformation\n",
    "def skewFix (df):\n",
    "    import numpy as np\n",
    "    from scipy.stats import skew\n",
    "    \n",
    "    def replace_inf_with_zero(df):\n",
    "        \n",
    "    # Replace infinite values with 0\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    df_transform = df.copy()\n",
    "    \n",
    "    for col in df_transform.columns:\n",
    "        if df_transform[col].dtype ==np.number:\n",
    "            sk  = skew(df_transform[col])\n",
    "            min_ = df_transform[col].min()\n",
    "            if (sk > 1.5) & (min_ >0):\n",
    "                df_transform[col] = np.log(df_transform[col])\n",
    "\n",
    "            if (sk > 1.5) & (min_ <=0):\n",
    "                df_transform[col] = np.cbrt(df_transform[col])\n",
    "\n",
    "    df_transform = replace_inf_with_zero(df_transform)\n",
    "    return(df_transform)\n",
    "\n",
    "def skewTest (df):\n",
    "    import numpy as np\n",
    "    from scipy.stats import skew\n",
    "    \n",
    "    sk = skew(df)\n",
    "    \n",
    "    df_skew = pd.DataFrame(data = sk,\n",
    "                           index = df.columns,)\n",
    "    return(df_skew)\n",
    "\n",
    "\n",
    "def standSector (data):\n",
    "    quarter_list = data.reset_index()['date'].dt.quarter.unique()\n",
    "    year_list = data.reset_index()['date'].dt.year.unique()\n",
    "    sector_list = data.reset_index()['sector'].unique()\n",
    "    data_norm = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for quarter in quarter_list:\n",
    "            for sector in sector_list:\n",
    "                query_year = data.index.get_level_values(1).year == year\n",
    "                query_quarter = data.index.get_level_values(1).quarter == quarter\n",
    "                query_sector = data.sector == sector\n",
    "                df_temp = data.loc[(query_year) & (query_quarter) & (query_sector)].copy()\n",
    "                if len(df_temp)>0:\n",
    "                    df_quantile = ft_extract.quantile_transform_df(df_temp,n_quantiles=100, random_state=0)\n",
    "\n",
    "                    data_norm = pd.concat([data_norm,df_quantile],\n",
    "                                          axis=0)\n",
    "    return(data_norm)\n",
    "\n",
    "def standQuarter (data):\n",
    "    quarter_list = data.reset_index()['date'].dt.quarter.unique()\n",
    "    year_list = data.reset_index()['date'].dt.year.unique()\n",
    "    \n",
    "    data_norm = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for quarter in quarter_list:\n",
    "            query_year = data.index.get_level_values(1).year == year\n",
    "            query_quarter = data.index.get_level_values(1).quarter == quarter\n",
    "            df_temp = data.loc[(query_year) & (query_quarter) ].copy()\n",
    "            \n",
    "            if len(df_temp)>0:\n",
    "                df_quantile = ft_extract.quantile_transform_df(df_temp,n_quantiles=100, random_state=0)\n",
    "\n",
    "                data_norm = pd.concat([data_norm,df_quantile],\n",
    "                                      axis=0)\n",
    "    return(data_norm)\n",
    "\n",
    "\n",
    "def calcRatios (data):\n",
    "    #calculate current ratio\n",
    "    \n",
    "    #liquidity Ratios\n",
    "    data['currentRatio'] = data['totalCurrentAssets'] / data['totalCurrentLiabilities']\n",
    "    data['currentRatio'] = data['currentRatio'].fillna(0)\n",
    "    \n",
    "    data['quickRatio'] = (data['cashAndCashEquivalents'] + data['shortTermInvestments'] + data['accountsReceivables'] )/ data['totalCurrentLiabilities']\n",
    "    data['quickRatio'] = data['quickRatio'].fillna(0)\n",
    "    \n",
    "    data['cashRatio'] =  data['cashAndCashEquivalents'] / data['totalCurrentLiabilities'] \n",
    "    data['cashRatio'] = data['cashRatio'].fillna(0)\n",
    "    \n",
    "    #debt Ratios\n",
    "    data['debtRatio'] =  data['totalLiabilities'] / data['totalAssets'] \n",
    "    data['debtRatio'] = data['debtRatio'].fillna(0)\n",
    "    \n",
    "    data['debtEquityRatio']= data['totalDebt']/ data['totalEquity']\n",
    "    data['debtEquityRatio'] = data['debtEquityRatio'].fillna(0)\n",
    "    \n",
    "    #profiitability\n",
    "    data['grossProfitMargin'] = data['grossProfit'] / data['revenue']\n",
    "    data['debtEquityRatio'] = data['debtEquityRatio'].fillna(0)\n",
    "    \n",
    "    data['operatingProfitMargin'] = data['operatingIncome'] / data['revenue']\n",
    "    data['operatingProfitMargin'] = data['operatingProfitMargin'].fillna(0)\n",
    "    \n",
    "    data['pretaxProfitMargin'] = data['incomeBeforeTax'] /  data['revenue'] \n",
    "    data['pretaxProfitMargin'] = data['pretaxProfitMargin'].fillna(0)\n",
    "    \n",
    "    data['returnOnAssets'] = data['netIncome'] / data['totalAssets']\n",
    "    data['returnOnAssets'] = data['returnOnAssets'].fillna(0)\n",
    "    \n",
    "    data['returnOnAssets'] = data['netIncome'] / data['totalAssets']\n",
    "    data['returnOnAssets'] = data['returnOnAssets'].fillna(0)\n",
    "    \n",
    "    data['revenuePerShare'] = data['revenue'] / data['numberOfShares']\n",
    "    data['revenuePerShare'] = data['revenuePerShare'].fillna(0)\n",
    "    data = replace_inf_with_zero(data)\n",
    "    return(data)\n",
    "\n",
    "if 'alpha' in df_target.columns:\n",
    "    df_target = df_target.drop(columns= 'alpha')\n",
    "    \n",
    "if 'beta' in df_target.columns:\n",
    "    df_target = df_target.drop(columns= 'beta')\n",
    "    \n",
    "df_model = gen_DFModel(df_imput,\n",
    "                       df_target)\n",
    "\n",
    "\n",
    "    \n",
    "#set major columns\n",
    "target_cols = df_model.columns[df_model.columns.str.contains('Target')]\n",
    "\n",
    "leakage_cols = [ 'futdate','close',\n",
    "       'futClose', 'futdate', 's&p', 'beta', 'fut_s&p', 'fut_s&p_date',\n",
    "       'fut_beta', 'futReturns', 'futMktReturns','excessReturns', 'Target_0',\n",
    "       'Target_0.05', 'Target_0.15']\n",
    "\n",
    "growth_cols = ['YoY_costOfRevenue', 'YoY_grossProfit', 'YoY_costAndExpenses',\n",
    "       'YoY_ebitda', 'YoY_operatingIncome', 'YoY_netIncome', 'YoY_eps']\n",
    "\n",
    "#set columns\n",
    "ratio_cols = df_model.columns[(df_model.columns.str.contains('Ratio'))]\n",
    "shares_cols = df_model.columns[(df_model.columns.str.contains('Share'))]\n",
    "margin_cols = df_model.columns[(df_model.columns.str.contains('Margin'))]\n",
    "turnover_cols = df_model.columns[(df_model.columns.str.contains('Turnover'))]\n",
    "dividend_cols = df_model.columns[(df_model.columns.str.contains('dividend'))]\n",
    "\n",
    "\n",
    "all_colls = [*ratio_cols, *shares_cols, *margin_cols,*turnover_cols,*dividend_cols]\n",
    "all_colls = set(all_colls)\n",
    "\n",
    "# df_sector = pd.read_csv(f'{input_path}\\companyInfo_sector.csv').drop(columns = 'Unnamed: 0')\n",
    "print('Nr of rows:')\n",
    "print(df_model.shape[0])\n",
    "\n",
    "#perform incoherence checkinng \n",
    "df_model = perform_Incoherence(df_model)\n",
    "\n",
    "\n",
    "##FEATURE ENGINEER 2\n",
    "#generate categorical features\n",
    "df_model = gen_categorical(df_model).reset_index().set_index(['symbol','date',\n",
    "                                                              'fillingDate','year',\n",
    "                                                              'quarter'])\n",
    "\n",
    "\n",
    "#create marketcap and marketcap categoricalb \n",
    "df_model = gen_categorical(df_model)\n",
    "\n",
    "\n",
    " #apply one hot encoder n sector and industry\n",
    "df_enc = oneHotEncoder(df_model[['sector','industry']])\n",
    "\n",
    "#concat with main data\n",
    "df_model = pd.concat([df_model,\n",
    "                    df_enc],\n",
    "                    axis=1)\n",
    "\n",
    "#gen binary features\n",
    "df_model = get_Binaries(df_model)\n",
    "#remove excess returns higher than 150%\n",
    "df_model = df_model.loc[df_model['excessReturns']<1.5]\n",
    "\n",
    "#define bin columns\n",
    "bin_cols =['Target_0',\n",
    "       'Target_0.05', 'Target_0.15','sector_priceEarningsRatio_bin', 'netDebt_bin',\n",
    "       'netIncome_bin', 'netProfitMargin_bin', 'mainSector']\n",
    "\n",
    "\n",
    "df_model[bin_cols] = df_model[bin_cols].astype(bool)\n",
    "\n",
    "bin_cols_ =['sector_priceEarningsRatio_bin', 'netDebt_bin',\n",
    "       'netIncome_bin', 'netProfitMargin_bin', 'mainSector']\n",
    "\n",
    "\n",
    "if 'index' in df_model.columns:\n",
    "    df_model = df_model.drop(columns = 'index')\n",
    "\n",
    "df_model = calcRatios(df_model)\n",
    "\n",
    "\n",
    "nr_obs = df_model.shape[0]\n",
    "n_cols = len(df_model.columns)\n",
    "print(f'Nr of observations : {nr_obs}')\n",
    "print(f'Nr of columns : {n_cols}')\n",
    "\n",
    "#create df skew\n",
    "df_model_skew = skewFix(df_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9976069974694686 of the data remained \n",
      "0.9976069974694686 of the data remained \n",
      "Nr of observations : 72538\n",
      "Nr of columns : 382\n"
     ]
    }
   ],
   "source": [
    "def dropDuplicates(df):\n",
    "    init_len = len(df)\n",
    "    df = df.reset_index().drop_duplicates(['symbol','fillingDate']).set_index(['symbol','date','fillingDate','year','quarter'])\n",
    "    \n",
    "    fin_len = len(df)\n",
    "    \n",
    "    print(f'{fin_len/init_len} of the data remained ')\n",
    "    return(df)\n",
    "\n",
    "\n",
    "df_model = dropDuplicates(df_model)\n",
    "df_model_skew = dropDuplicates(df_model_skew)\n",
    "\n",
    "nr_obs = df_model.shape[0]\n",
    "n_cols = len(df_model.columns)\n",
    "print(f'Nr of observations : {nr_obs}')\n",
    "print(f'Nr of columns : {n_cols}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_full_path = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation_v2.csv'\n",
    "\n",
    "data = pd.read_csv(df_full_path)\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['fillingDate'] = pd.to_datetime(data['fillingDate'])\n",
    "data = data.set_index(['symbol','date','fillingDate','year','quarter'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data considering Quarter and Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "leakage_cols = ['alpha', 'futDate', 'excessReturns',\n",
    "       'Target_0', 'Target_0.05', 'Target_0.15']\n",
    "\n",
    "data_sector = standSector(df_model.drop(columns=leakage_cols))\n",
    "data_quarter = standQuarter(df_model.drop(columns=leakage_cols))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- excessReturns corresponds to the Target calculated using the adjusted CAPM formula  \n",
    "- futAlpha corresponds to the Target calculated using the original CAPM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store on computer memory\n",
    "df_full_path = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation.csv'\n",
    "df_full_path_skew = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation_Skew.csv'\n",
    "df_model.to_csv(df_full_path)\n",
    "df_model_skew.to_csv(df_full_path_skew)\n",
    "\n",
    "#store on computer memory\n",
    "path_sectorStand = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation_sectorStand.csv'\n",
    "path_quarterStand = r'D:\\Desktop\\Long-Term-Model\\Data\\LongTerm-DataPreparation_quarterStand.csv'\n",
    "\n",
    "data_sector.to_csv(path_sectorStand)\n",
    "data_quarter.to_csv(path_quarterStand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
